{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python to Integrate MongoDB Data into an ETL Process\n",
    "This notebook demonstrates the setup of an ETL (Extract, Transform, Load) pipeline.\n",
    "\n",
    "In this project, I will build upon the **MyShop** dimensional database; however, you will be integrating new data sourced from an instance of MongoDB. The new data will be concerned with new business processes; inventory and purchasing. I will continue to interact with both the source systems (MongoDB and MySQL), and the destination system (the MyShop data warehouse) from a remote client running Python (Jupyter Notebooks). \n",
    "\n",
    "I fetch data into Pandas DataFrames, perform all the necessary transformations in-memory on the client, and then push the newly transformed DataFrame to the RDBMS data warehouse using a Pandas function that will create the table and fill it with data with a single operation.\n",
    "\n",
    "### Prerequisites:\n",
    "This notebook uses the PyMongo database connectivity library to connect to MySQL databases; therefore, you must have first installed that libary into your python environment by executing the following command in a Terminal window.\n",
    "\n",
    "- `python -m pip install pymongo[srv]`\n",
    "\n",
    "#### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL Alchemy Version: 2.0.34\n",
      "Running PyMongo Version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare & Assign Connection Variables for the MongoDB Server, the MySQL Server & Databases with which You'll be Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "# Example setup of logging for the notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to get MongoDB client\n",
    "def get_mongo_client(host: str, port: int, username: str = None, password: str = None, db_name: str = None) -> MongoClient:\n",
    "    \"\"\"Initialize MongoDB client for a remote database.\"\"\"\n",
    "    connection_string = f\"mongodb://{host}\"\n",
    "    if username and password:\n",
    "        connection_string = f\"mongodb://{username}:{password}@{host}\"\n",
    "\n",
    "    try:\n",
    "        client = MongoClient(connection_string)\n",
    "        if db_name:\n",
    "            client = client[db_name]\n",
    "        logger.info(\"MongoDB client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize MongoDB client: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# SQL connection\n",
    "def get_sql_connection(host: str, user: str, password: str, db: str):\n",
    "    \"\"\"Initialize SQL connection.\"\"\"\n",
    "    conn = pymysql.connect(host=host, user=user, password=password, db=db)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Data directory set to: /Users/mac/Downloads/data-warehouse-project/data\n"
     ]
    }
   ],
   "source": [
    "# Set the path of the current working directory and append 'data' directory\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "logger.info(f\"Data directory set to: {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define JSON files for MongoDB collections\n",
    "json_files = {\n",
    "    \"sales_orders\": 'StoreSales.json',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mongo_collections(client: MongoClient, db_name: str, data_dir: str, json_files: dict):\n",
    "    \"\"\"Load JSON data into MongoDB collections.\"\"\"\n",
    "    db = client[db_name]\n",
    "    for collection_name, file_name in json_files.items():\n",
    "        file_path = os.path.abspath(os.path.join(data_dir, file_name))\n",
    "        \n",
    "        # Load JSON data and insert into MongoDB\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list): \n",
    "                    db[collection_name].insert_many(data)\n",
    "                    logger.info(f\"Inserted {len(data)} documents into '{collection_name}' collection.\")\n",
    "                else:\n",
    "                    db[collection_name].insert_one(data)\n",
    "                    logger.info(f\"Inserted a single document into '{collection_name}' collection.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data for collection '{collection_name}': {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection arguments (example)\n",
    "mongodb_args = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 27017,\n",
    "    \"username\": \"mikelangelo1\",\n",
    "    \"password\": \"password123\",\n",
    "    \"db_name\": \"data_ware_house\"\n",
    "}\n",
    "\n",
    "# Initialize the MongoDB client\n",
    "client = get_mongo_client(\n",
    "    host=mongodb_args[\"host\"],\n",
    "    port=mongodb_args[\"port\"],\n",
    "    username=mongodb_args.get(\"username\"),\n",
    "    password=mongodb_args.get(\"password\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate MongoDB with Source Data\n",
    "You only need to run this cell once; however, the operation is *idempotent*.  In other words, it can be run multiple times without changing the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Inserted 51291 documents into 'sales_orders' collection.\n"
     ]
    }
   ],
   "source": [
    "# Load data into MongoDB collections\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extractor\n",
    "This class provides mock methods to:\n",
    "\n",
    "Extract data from a MongoDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB client initialized.\n",
      "INFO:__main__:MongoDB connection established to database: data_ware_house\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB Data:\n",
      "                             _id Row ID         Order ID  Order Date  \\\n",
      "0       673167cbbdc428b0f7839e83  32298   CA-2012-124891  31-07-2012   \n",
      "1       673167cbbdc428b0f7839e84  26341    IN-2013-77878  05-02-2013   \n",
      "2       673167cbbdc428b0f7839e85  25330    IN-2013-71249  17-10-2013   \n",
      "3       673167cbbdc428b0f7839e86  13524  ES-2013-1579342  28-01-2013   \n",
      "4       673167cbbdc428b0f7839e87  47221     SG-2013-4320  05-11-2013   \n",
      "...                          ...    ...              ...         ...   \n",
      "256450  6747d510c608a76b1ff52dfd  35398   US-2014-102288  20-06-2014   \n",
      "256451  6747d510c608a76b1ff52dfe  40470   US-2013-155768  02-12-2013   \n",
      "256452  6747d510c608a76b1ff52dff   9596   MX-2012-140767  18-02-2012   \n",
      "256453  6747d510c608a76b1ff52e00   6147   MX-2012-134460  22-05-2012   \n",
      "256454  6747d510c608a76b1ff52e01                     NaN         NaN   \n",
      "\n",
      "         Ship Date       Ship Mode Customer ID     Customer Name      Segment  \\\n",
      "0       31-07-2012        Same Day    RH-19495       Rick Hansen     Consumer   \n",
      "1       07-02-2013    Second Class    JR-16210     Justin Ritter    Corporate   \n",
      "2       18-10-2013     First Class    CR-12730      Craig Reiter     Consumer   \n",
      "3       30-01-2013     First Class    KM-16375  Katherine Murray  Home Office   \n",
      "4       06-11-2013        Same Day     RH-9495       Rick Hansen     Consumer   \n",
      "...            ...             ...         ...               ...          ...   \n",
      "256450  24-06-2014  Standard Class    ZC-21910  Zuschuss Carroll     Consumer   \n",
      "256451  02-12-2013        Same Day    LB-16795    Laurel Beltran  Home Office   \n",
      "256452  22-02-2012  Standard Class    RB-19795        Ross Baird  Home Office   \n",
      "256453  26-05-2012    Second Class    MC-18100     Mick Crebagga     Consumer   \n",
      "256454         NaN             NaN         NaN               NaN          NaN   \n",
      "\n",
      "                 City  ...        Product ID         Category Sub-Category  \\\n",
      "0       New York City  ...   TEC-AC-10003033       Technology  Accessories   \n",
      "1          Wollongong  ...   FUR-CH-10003950        Furniture       Chairs   \n",
      "2            Brisbane  ...   TEC-PH-10004664       Technology       Phones   \n",
      "3              Berlin  ...   TEC-PH-10004583       Technology       Phones   \n",
      "4               Dakar  ...  TEC-SHA-10000501       Technology      Copiers   \n",
      "...               ...  ...               ...              ...          ...   \n",
      "256450        Houston  ...   OFF-AP-10002906  Office Supplies   Appliances   \n",
      "256451         Oxnard  ...   OFF-EN-10001219  Office Supplies    Envelopes   \n",
      "256452       Valinhos  ...   OFF-BI-10000806  Office Supplies      Binders   \n",
      "256453       Tipitapa  ...   OFF-PA-10004155  Office Supplies        Paper   \n",
      "256454            NaN  ...               NaN              NaN          NaN   \n",
      "\n",
      "                                             Product Name     Sales Quantity  \\\n",
      "0       Plantronics CS510 - Over-the-Head monaural Wir...   2309.65        7   \n",
      "1               Novimex Executive Leather Armchair, Black  3709.395        9   \n",
      "2                       Nokia Smart Phone, with Caller ID  5175.171        9   \n",
      "3                          Motorola Smart Phone, Cordless   2892.51        5   \n",
      "4                          Sharp Wireless Fax, High-Speed   2832.96        8   \n",
      "...                                                   ...       ...      ...   \n",
      "256450  Hoover Replacement Belt for Commercial Guardsm...     0.444        1   \n",
      "256451       #10- 4 1/8\" x 9 1/2\" Security-Tint Envelopes     22.92        3   \n",
      "256452                            Acco Index Tab, Economy     13.44        2   \n",
      "256453            Eaton Computer Printout Paper, 8.5 x 11     61.38        3   \n",
      "256454                                                NaN       NaN      NaN   \n",
      "\n",
      "       Discount    Profit Shipping Cost Order Priority  \n",
      "0             0  762.1845        933.57       Critical  \n",
      "1           0.1  -288.765        923.63       Critical  \n",
      "2           0.1   919.971        915.49         Medium  \n",
      "3           0.1    -96.54        910.16         Medium  \n",
      "4             0    311.52        903.04       Critical  \n",
      "...         ...       ...           ...            ...  \n",
      "256450      0.8     -1.11          0.01         Medium  \n",
      "256451        0   11.2308          0.01           High  \n",
      "256452        0       2.4             0         Medium  \n",
      "256453        0       1.8             0           High  \n",
      "256454      NaN       NaN           NaN            NaN  \n",
      "\n",
      "[256455 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "class DatabaseConnection:\n",
    "    def __init__(self, mongodb_args: Dict):\n",
    "        \"\"\"Initialize the DatabaseConnection with MongoDB client parameters.\"\"\"\n",
    "        self.client = self.get_mongo_client(mongodb_args)\n",
    "        self.db = None\n",
    "        self.db_name = mongodb_args.get('db_name', 'default_db')\n",
    "\n",
    "    def get_mongo_connection(self):\n",
    "        \"\"\"Return a MongoDB connection using the provided config.\"\"\"\n",
    "        try:\n",
    "            # Connect to MongoDB and access the specified database\n",
    "            self.db = self.client[self.db_name]\n",
    "            logger.info(f\"MongoDB connection established to database: {self.db_name}\")\n",
    "            return self.db  \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error connecting to MongoDB: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def close_connections(self):\n",
    "        \"\"\"Close MongoDB connection.\"\"\"\n",
    "        try:\n",
    "            if self.client:\n",
    "                self.client.close()\n",
    "                logger.info(\"MongoDB connection closed.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error closing MongoDB connection: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_mongo_client(self, mongodb_args: Dict) -> MongoClient:\n",
    "        \"\"\"Initialize and return MongoDB client.\"\"\"\n",
    "        try:\n",
    "            host = mongodb_args.get(\"host\", \"localhost\")\n",
    "            port = mongodb_args.get(\"port\", 27017)\n",
    "            username = mongodb_args.get(\"username\")\n",
    "            password = mongodb_args.get(\"password\")\n",
    "\n",
    "            if username and password:\n",
    "                client = MongoClient(host, port, username=username, password=password)\n",
    "            else:\n",
    "                client = MongoClient(host, port)\n",
    "\n",
    "            logger.info(\"MongoDB client initialized.\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing MongoDB client: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, db_connection: DatabaseConnection):\n",
    "        \"\"\"Initialize DataExtractor with the given database connection.\"\"\"\n",
    "        self.db_conn = db_connection\n",
    "        \n",
    "    def extract_from_mongodb(self, collection: str, query: Dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from MongoDB collection.\"\"\"\n",
    "        try:\n",
    "            mongo_db = self.db_conn.get_mongo_connection()\n",
    "            data = mongo_db[collection].find(query or {})  # Find all documents or filtered by query\n",
    "            return pd.DataFrame(list(data))  # Convert to DataFrame\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from MongoDB collection '{collection}': {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def extract_from_api(self, endpoint: str, params: Dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from a REST API.\"\"\"\n",
    "        try:\n",
    "            session = self.db_conn.get_api_session()  \n",
    "            api_config = self.db_conn.config['api']\n",
    "            response = session.get(f\"{api_config['base_url']}/{endpoint}\", params=params)\n",
    "            response.raise_for_status()\n",
    "            return pd.DataFrame(response.json())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from API endpoint '{endpoint}': {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "db_conn = DatabaseConnection(mongodb_args)\n",
    "extractor = DataExtractor(db_conn)\n",
    "\n",
    "try:\n",
    "    df_mongo = extractor.extract_from_mongodb(\"sales_orders\")\n",
    "    print(\"MongoDB Data:\")\n",
    "    print(df_mongo)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in MongoDB extraction test: {str(e)}\")\n",
    "\n",
    "# Note: For `extract_from_api`, replace with an actual endpoint or mock response as required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "This class provides the implementation of load_to_warehouse, simulating loading a DataFrame into a data warehouse. It connects to the MSQL database and load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "class MsqlDatabaseConnection:\n",
    "    def __init__(self, username: str, password: str, host: str = \"localhost\", port: int = 3306, db_name: str = \"myshop\"):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.db_name = db_name\n",
    "\n",
    "    def get_sqlalchemy_engine(self) -> Engine:\n",
    "        \"\"\"Return a SQLAlchemy engine connected to MySQL database.\"\"\"\n",
    "        try:\n",
    "            connection_uri = f\"mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.db_name}\"\n",
    "            engine = create_engine(connection_uri)\n",
    "            logger.info(\"Successfully connected to MySQL database.\")\n",
    "            return engine\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error connecting to MySQL: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_query_result(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute a query and return the result as a DataFrame.\"\"\"\n",
    "        try:\n",
    "            engine = self.get_sqlalchemy_engine()\n",
    "            result = pd.read_sql(query, engine)\n",
    "            logger.info(f\"Query executed successfully: {query}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing query: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# DataLoader class\n",
    "class DataLoader:\n",
    "    def __init__(self, db_connection: MsqlDatabaseConnection):\n",
    "        self.db_conn = db_connection\n",
    "    \n",
    "    def load_to_warehouse(self, df: pd.DataFrame, table_name: str, if_exists: str = 'append') -> None:\n",
    "        \"\"\"Load DataFrame to data warehouse.\"\"\"\n",
    "        try:\n",
    "            engine = self.db_conn.get_sqlalchemy_engine()\n",
    "            df.to_sql(\n",
    "                name=table_name,\n",
    "                con=engine,\n",
    "                if_exists=if_exists,\n",
    "                index=False,\n",
    "                chunksize=1000\n",
    "            )\n",
    "            logger.info(f\"Successfully loaded {len(df)} rows to {table_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data to warehouse: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "db_connection = MsqlDatabaseConnection(\n",
    "    username=\"root\",  # Replace with your MySQL username\n",
    "    password=\"Akinolami6650!\",  # Replace with your MySQL password\n",
    "    db_name=\"myshop\"           # The name of your database\n",
    ")\n",
    "\n",
    "# Create an instance of DataLoader\n",
    "data_loader = DataLoader(db_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate MongoDB with Source Data\n",
    "You only need to run this cell once; however, the operation is *idempotent*.  In other words, it can be run multiple times without changing the end result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Create and Populate the New Dimension Tables\n",
    "#### 1.1. Extract Data from the Source MongoDB Collections Into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB connection established to database: data_ware_house\n",
      "INFO:__main__:Sales order data extracted successfully.\n",
      "ERROR:__main__:Invalid data in numeric columns:\n",
      "                             _id Row ID Order ID Order Date Ship Date  \\\n",
      "51290   673167cbbdc428b0f78466dd             NaN        NaN       NaN   \n",
      "102581  67316b18bdc428b0f7852f39             NaN        NaN       NaN   \n",
      "153872  67316b4fbdc428b0f785f795             NaN        NaN       NaN   \n",
      "205163  67316ec2bdc428b0f786bff5             NaN        NaN       NaN   \n",
      "256454  6747d510c608a76b1ff52e01             NaN        NaN       NaN   \n",
      "\n",
      "       Ship Mode Customer ID Customer Name Segment City  ... Product ID  \\\n",
      "51290        NaN         NaN           NaN     NaN  NaN  ...        NaN   \n",
      "102581       NaN         NaN           NaN     NaN  NaN  ...        NaN   \n",
      "153872       NaN         NaN           NaN     NaN  NaN  ...        NaN   \n",
      "205163       NaN         NaN           NaN     NaN  NaN  ...        NaN   \n",
      "256454       NaN         NaN           NaN     NaN  NaN  ...        NaN   \n",
      "\n",
      "       Category Sub-Category Product Name Sales Quantity Discount Profit  \\\n",
      "51290       NaN          NaN          NaN   NaN      NaN      NaN    NaN   \n",
      "102581      NaN          NaN          NaN   NaN      NaN      NaN    NaN   \n",
      "153872      NaN          NaN          NaN   NaN      NaN      NaN    NaN   \n",
      "205163      NaN          NaN          NaN   NaN      NaN      NaN    NaN   \n",
      "256454      NaN          NaN          NaN   NaN      NaN      NaN    NaN   \n",
      "\n",
      "       Shipping Cost  Order Priority  \n",
      "51290            NaN             NaN  \n",
      "102581           NaN             NaN  \n",
      "153872           NaN             NaN  \n",
      "205163           NaN             NaN  \n",
      "256454           NaN             NaN  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "INFO:__main__:Dropped 5 rows with invalid numeric data.\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Total Amount'] = df['Quantity'] * df['Sales']  # Calculate total sales amount\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Discount Amount'] = df['Total Amount'] * df['Discount']  # Calculate discount amount\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Final Amount'] = df['Total Amount'] - df['Discount Amount']  # Calculate final sales amount\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d-%m-%Y')\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%d-%m-%Y')\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Order Year'] = df['Order Date'].dt.year\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Order Month'] = df['Order Date'].dt.month\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Order Quarter'] = df['Order Date'].dt.quarter\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Ship Year'] = df['Ship Date'].dt.year\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Ship Month'] = df['Ship Date'].dt.month\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_25754/943697967.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Ship Quarter'] = df['Ship Date'].dt.quarter\n",
      "INFO:__main__:Sales data transformed successfully.\n",
      "INFO:__main__:Sales data transformed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Sales Data:\n"
     ]
    }
   ],
   "source": [
    "class DataExtractor:\n",
    "    def __init__(self, db_connection: DatabaseConnection):\n",
    "        self.db_conn = db_connection\n",
    "        \n",
    "    def extract_from_mongodb(self, collection: str, query: Dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from MongoDB collection.\"\"\"\n",
    "        try:\n",
    "            mongo_db = self.db_conn.get_mongo_connection(\"data_ware_house\")\n",
    "            data = mongo_db[collection].find(query if query else {})\n",
    "            return pd.DataFrame(list(data))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from MongoDB: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# DataTransformer class\n",
    "class DataTransformer:\n",
    "    @staticmethod\n",
    "    def clean_customer_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and transform customer data.\"\"\"\n",
    "        try:\n",
    "            # Remove duplicates\n",
    "            df = df.drop_duplicates()\n",
    "            \n",
    "            # Handle missing values (example with customer data fields)\n",
    "            df['Customer Name'] = df['Customer Name'].fillna('Unknown')\n",
    "            df['City'] = df['City'].fillna('Unknown')\n",
    "            df['State'] = df['State'].fillna('Unknown')\n",
    "            df['Country'] = df['Country'].fillna('Unknown')\n",
    "            df['Postal Code'] = df['Postal Code'].fillna('')\n",
    "            \n",
    "            # Standardize customer-related columns (e.g., postal codes as strings)\n",
    "            df['Postal Code'] = df['Postal Code'].apply(lambda x: str(x).zfill(5))  # Fill leading zeros if necessary\n",
    "            \n",
    "            logger.info(\"Customer data cleaned successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning customer data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_sales_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform sales data.\"\"\"\n",
    "        try:\n",
    "            # Convert numeric columns to the correct type\n",
    "            df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
    "            df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce')\n",
    "            df['Discount'] = pd.to_numeric(df['Discount'], errors='coerce')\n",
    "\n",
    "            invalid_rows = df[df[['Quantity', 'Sales', 'Discount']].isna().any(axis=1)]\n",
    "            if not invalid_rows.empty:\n",
    "                logger.error(f\"Invalid data in numeric columns:\\n{invalid_rows}\")\n",
    "                df = df.dropna(subset=['Quantity', 'Sales', 'Discount'])\n",
    "                logger.info(f\"Dropped {len(invalid_rows)} rows with invalid numeric data.\")\n",
    "\n",
    "            # Calculate derived columns\n",
    "            df['Total Amount'] = df['Quantity'] * df['Sales']  # Calculate total sales amount\n",
    "            df['Discount Amount'] = df['Total Amount'] * df['Discount']  # Calculate discount amount\n",
    "            df['Final Amount'] = df['Total Amount'] - df['Discount Amount']  # Calculate final sales amount\n",
    "\n",
    "            # Convert dates\n",
    "            df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d-%m-%Y')\n",
    "            df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%d-%m-%Y')\n",
    "\n",
    "            # Add time dimensions (Year, Month, Quarter)\n",
    "            df['Order Year'] = df['Order Date'].dt.year\n",
    "            df['Order Month'] = df['Order Date'].dt.month\n",
    "            df['Order Quarter'] = df['Order Date'].dt.quarter\n",
    "            df['Ship Year'] = df['Ship Date'].dt.year\n",
    "            df['Ship Month'] = df['Ship Date'].dt.month\n",
    "            df['Ship Quarter'] = df['Ship Date'].dt.quarter\n",
    "\n",
    "            logger.info(\"Sales data transformed successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error transforming sales data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "try:\n",
    "    df_sales = extractor.extract_from_mongodb(\"sales_orders\")\n",
    "    logger.info(\"Sales order data extracted successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error extracting sales data: {str(e)}\")\n",
    "\n",
    "# Apply transformations to the extracted sales data\n",
    "try:\n",
    "    df_transformed_sales = DataTransformer.transform_sales_data(df_sales)\n",
    "    logger.info(\"Sales data transformed successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error transforming sales data: {str(e)}\")\n",
    "\n",
    "# Display transformed data\n",
    "print(\"Transformed Sales Data:\")\n",
    "# print(df_transformed_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Analysis by Customer Segment and Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: \n",
      "SELECT \n",
      "    c.job_title as Segment,\n",
      "    CONCAT(YEAR(o.order_date), ' Q', QUARTER(o.order_date)) as Year_Quarter,\n",
      "    COUNT(DISTINCT o.id) as Total_Orders,\n",
      "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Sales,\n",
      "    AVG(od.discount) as Avg_Discount,\n",
      "    SUM(od.quantity) as Units_Sold\n",
      "FROM orders o\n",
      "JOIN customers c ON o.customer_id = c.id\n",
      "JOIN order_details od ON o.id = od.order_id\n",
      "GROUP BY \n",
      "    c.job_title, \n",
      "    CONCAT(YEAR(o.order_date), ' Q', QUARTER(o.order_date))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Analysis by Customer Segment and Quarter:\n",
      "                     Segment Year_Quarter  Total_Orders  Total_Sales  \\\n",
      "0       Accounting Assistant      2006 Q2             2      3625.25   \n",
      "1                      Owner      2006 Q1             2     15474.75   \n",
      "2                      Owner      2006 Q2             1       736.00   \n",
      "3         Purchasing Manager      2006 Q1            10     19731.00   \n",
      "4         Purchasing Manager      2006 Q2            17     21337.00   \n",
      "5  Purchasing Representative      2006 Q1             3      3481.00   \n",
      "6  Purchasing Representative      2006 Q2             5      3752.00   \n",
      "\n",
      "   Avg_Discount  Units_Sold  \n",
      "0           0.0       175.0  \n",
      "1           0.0       375.0  \n",
      "2           0.0        40.0  \n",
      "3           0.0       842.0  \n",
      "4           0.0       997.0  \n",
      "5           0.0       330.0  \n",
      "6           0.0       183.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sales_by_segment_query = \"\"\"\n",
    "SELECT \n",
    "    c.job_title as Segment,\n",
    "    CONCAT(YEAR(o.order_date), ' Q', QUARTER(o.order_date)) as Year_Quarter,\n",
    "    COUNT(DISTINCT o.id) as Total_Orders,\n",
    "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Sales,\n",
    "    AVG(od.discount) as Avg_Discount,\n",
    "    SUM(od.quantity) as Units_Sold\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.id\n",
    "JOIN order_details od ON o.id = od.order_id\n",
    "GROUP BY \n",
    "    c.job_title, \n",
    "    CONCAT(YEAR(o.order_date), ' Q', QUARTER(o.order_date))\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_segment_analysis = db_connection.get_query_result(sales_by_segment_query)\n",
    "    print(\"Sales Analysis by Customer Segment and Quarter:\")\n",
    "    print(df_segment_analysis)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in segment analysis query: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Category Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: \n",
      "SELECT \n",
      "    p.category as Category,\n",
      "    COUNT(DISTINCT od.order_id) as Number_of_Orders,\n",
      "    SUM(od.quantity) as Total_Units_Sold,\n",
      "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Revenue,\n",
      "    AVG(od.discount) as Avg_Discount_Rate,\n",
      "    SUM(od.quantity * od.unit_price)/SUM(od.quantity) as Avg_Unit_Price\n",
      "FROM products p\n",
      "JOIN order_details od ON p.id = od.product_id\n",
      "GROUP BY p.category\n",
      "ORDER BY Total_Revenue DESC;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Product Category Performance Analysis:\n",
      "                     Category  Number_of_Orders  Total_Units_Sold  \\\n",
      "0                   Beverages                11            1452.0   \n",
      "1             Jams, Preserves                 3             140.0   \n",
      "2          Dried Fruit & Nuts                 6             175.0   \n",
      "3              Dairy products                 2              90.0   \n",
      "4                       Soups                 4             290.0   \n",
      "5                      Sauces                 4              65.0   \n",
      "6                       Candy                 5             200.0   \n",
      "7                       Pasta                 3             110.0   \n",
      "8                 Canned Meat                 3             120.0   \n",
      "9   Canned Fruit & Vegetables                 1              40.0   \n",
      "10                 Condiments                 3              90.0   \n",
      "11        Baked Goods & Mixes                 5             105.0   \n",
      "12                        Oil                 1              25.0   \n",
      "13                     Grains                 1              40.0   \n",
      "\n",
      "    Total_Revenue  Avg_Discount_Rate  Avg_Unit_Price  \n",
      "0        38260.25                0.0       26.350034  \n",
      "1         5740.00                0.0       41.000000  \n",
      "2         3782.50                0.0       21.614286  \n",
      "3         3132.00                0.0       34.800000  \n",
      "4         2798.50                0.0        9.650000  \n",
      "5         2600.00                0.0       40.000000  \n",
      "6         2550.00                0.0       12.750000  \n",
      "7         2330.00                0.0       21.181818  \n",
      "8         2208.00                0.0       18.400000  \n",
      "9         1560.00                0.0       39.000000  \n",
      "10        1380.00                0.0       15.333333  \n",
      "11         982.00                0.0        9.352381  \n",
      "12         533.75                0.0       21.350000  \n",
      "13         280.00                0.0        7.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "product_performance_query = \"\"\"\n",
    "SELECT \n",
    "    p.category as Category,\n",
    "    COUNT(DISTINCT od.order_id) as Number_of_Orders,\n",
    "    SUM(od.quantity) as Total_Units_Sold,\n",
    "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Revenue,\n",
    "    AVG(od.discount) as Avg_Discount_Rate,\n",
    "    SUM(od.quantity * od.unit_price)/SUM(od.quantity) as Avg_Unit_Price\n",
    "FROM products p\n",
    "JOIN order_details od ON p.id = od.product_id\n",
    "GROUP BY p.category\n",
    "ORDER BY Total_Revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_product_performance = db_connection.get_query_result(product_performance_query)\n",
    "    print(\"\\nProduct Category Performance Analysis:\")\n",
    "    print(df_product_performance)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in product performance query: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Sales Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: \n",
      "SELECT \n",
      "    c.country_region as Country,\n",
      "    c.city as City,\n",
      "    COUNT(DISTINCT o.customer_id) as Unique_Customers,\n",
      "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Sales,\n",
      "    AVG(od.quantity * od.unit_price * (1 - od.discount)) as Avg_Order_Value,\n",
      "    SUM(od.quantity) as Total_Units_Sold\n",
      "FROM orders o\n",
      "JOIN customers c ON o.customer_id = c.id\n",
      "JOIN order_details od ON o.id = od.order_id\n",
      "GROUP BY c.country_region, c.city\n",
      "HAVING Total_Sales > 1000\n",
      "ORDER BY Total_Sales DESC;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Geographic Sales Distribution:\n",
      "   Country            City  Unique_Customers  Total_Sales  Avg_Order_Value  \\\n",
      "0      USA         Memphis                 1     15432.50      3858.125000   \n",
      "1      USA           Boise                 1     13800.00     13800.000000   \n",
      "2      USA       Milwaukee                 1      8007.50      1334.583333   \n",
      "3      USA        New York                 1      4949.00       707.000000   \n",
      "4      USA        Portland                 1      4683.00       780.500000   \n",
      "5      USA           Miami                 2      4644.75       663.535714   \n",
      "6      USA  Salt Lake City                 1      3786.50      1262.166667   \n",
      "7      USA          Denver                 1      2905.50       968.500000   \n",
      "8      USA       Las Vegas                 2      2695.00       673.750000   \n",
      "9      USA     Los Angelas                 1      2550.00       510.000000   \n",
      "10     USA         Seattle                 1      2410.75       602.687500   \n",
      "11     USA         Chicago                 2      2272.50       284.062500   \n",
      "\n",
      "    Total_Units_Sold  \n",
      "0              405.0  \n",
      "1              300.0  \n",
      "2              427.0  \n",
      "3              140.0  \n",
      "4              260.0  \n",
      "5              265.0  \n",
      "6              160.0  \n",
      "7              137.0  \n",
      "8              165.0  \n",
      "9              253.0  \n",
      "10             115.0  \n",
      "11             315.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "geographic_sales_query = \"\"\"\n",
    "SELECT \n",
    "    c.country_region as Country,\n",
    "    c.city as City,\n",
    "    COUNT(DISTINCT o.customer_id) as Unique_Customers,\n",
    "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Sales,\n",
    "    AVG(od.quantity * od.unit_price * (1 - od.discount)) as Avg_Order_Value,\n",
    "    SUM(od.quantity) as Total_Units_Sold\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.id\n",
    "JOIN order_details od ON o.id = od.order_id\n",
    "GROUP BY c.country_region, c.city\n",
    "HAVING Total_Sales > 1000\n",
    "ORDER BY Total_Sales DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_geographic_sales = db_connection.get_query_result(geographic_sales_query)\n",
    "    print(\"\\nGeographic Sales Distribution:\")\n",
    "    print(df_geographic_sales)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in geographic sales query: {str(e)}\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shipping Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: \n",
      "SELECT \n",
      "    s.company as Ship_Mode,\n",
      "    COUNT(DISTINCT o.id) as Total_Shipments,\n",
      "    AVG(DATEDIFF(o.shipped_date, o.order_date)) as Avg_Days_to_Ship,\n",
      "    SUM(o.shipping_fee) as Total_Shipping_Cost,\n",
      "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Order_Value,\n",
      "    (SUM(o.shipping_fee) / SUM(od.quantity * od.unit_price * (1 - od.discount))) * 100 as Shipping_Cost_Percentage\n",
      "FROM orders o\n",
      "JOIN shippers s ON o.shipper_id = s.id\n",
      "JOIN order_details od ON o.id = od.order_id\n",
      "WHERE o.shipped_date IS NOT NULL\n",
      "GROUP BY s.company\n",
      "ORDER BY Total_Shipments DESC;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shipping Performance Analysis:\n",
      "            Ship_Mode  Total_Shipments  Avg_Days_to_Ship  Total_Shipping_Cost  \\\n",
      "0  Shipping Company B               14            1.1667               1618.0   \n",
      "1  Shipping Company C               11            0.3333                479.0   \n",
      "2  Shipping Company A                7            3.6923                335.0   \n",
      "\n",
      "   Total_Order_Value  Shipping_Cost_Percentage  \n",
      "0           16078.50                 10.063128  \n",
      "1           24802.25                  1.931276  \n",
      "2            9593.50                  3.491948  \n"
     ]
    }
   ],
   "source": [
    "shipping_performance_query = \"\"\"\n",
    "SELECT \n",
    "    s.company as Ship_Mode,\n",
    "    COUNT(DISTINCT o.id) as Total_Shipments,\n",
    "    AVG(DATEDIFF(o.shipped_date, o.order_date)) as Avg_Days_to_Ship,\n",
    "    SUM(o.shipping_fee) as Total_Shipping_Cost,\n",
    "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Order_Value,\n",
    "    (SUM(o.shipping_fee) / SUM(od.quantity * od.unit_price * (1 - od.discount))) * 100 as Shipping_Cost_Percentage\n",
    "FROM orders o\n",
    "JOIN shippers s ON o.shipper_id = s.id\n",
    "JOIN order_details od ON o.id = od.order_id\n",
    "WHERE o.shipped_date IS NOT NULL\n",
    "GROUP BY s.company\n",
    "ORDER BY Total_Shipments DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_shipping_performance = db_connection.get_query_result(shipping_performance_query)\n",
    "    print(\"\\nShipping Performance Analysis:\")\n",
    "    print(df_shipping_performance)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in shipping performance query: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: SELECT * FROM myshop.employees;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Result:\n",
      "   id         company       last_name first_name              email_address  \\\n",
      "0   1  myshop Traders       Freehafer      Nancy    nancy@myshoptraders.com   \n",
      "1   2  myshop Traders         Cencini     Andrew   andrew@myshoptraders.com   \n",
      "2   3  myshop Traders           Kotas        Jan      jan@myshoptraders.com   \n",
      "3   4  myshop Traders       Sergienko     Mariya   mariya@myshoptraders.com   \n",
      "4   5  myshop Traders          Thorpe     Steven   steven@myshoptraders.com   \n",
      "5   6  myshop Traders         Neipper    Michael  michael@myshoptraders.com   \n",
      "6   7  myshop Traders            Zare     Robert   robert@myshoptraders.com   \n",
      "7   8  myshop Traders        Giussani      Laura    laura@myshoptraders.com   \n",
      "8   9  myshop Traders  Hellung-Larsen       Anne     anne@myshoptraders.com   \n",
      "\n",
      "               job_title business_phone     home_phone mobile_phone  \\\n",
      "0   Sales Representative  (123)555-0100  (123)555-0102         None   \n",
      "1  Vice President, Sales  (123)555-0100  (123)555-0102         None   \n",
      "2   Sales Representative  (123)555-0100  (123)555-0102         None   \n",
      "3   Sales Representative  (123)555-0100  (123)555-0102         None   \n",
      "4          Sales Manager  (123)555-0100  (123)555-0102         None   \n",
      "5   Sales Representative  (123)555-0100  (123)555-0102         None   \n",
      "6   Sales Representative  (123)555-0100  (123)555-0102         None   \n",
      "7      Sales Coordinator  (123)555-0100  (123)555-0102         None   \n",
      "8   Sales Representative  (123)555-0100  (123)555-0102         None   \n",
      "\n",
      "      fax_number         address      city state_province zip_postal_code  \\\n",
      "0  (123)555-0103  123 1st Avenue   Seattle             WA           99999   \n",
      "1  (123)555-0103  123 2nd Avenue  Bellevue             WA           99999   \n",
      "2  (123)555-0103  123 3rd Avenue   Redmond             WA           99999   \n",
      "3  (123)555-0103  123 4th Avenue  Kirkland             WA           99999   \n",
      "4  (123)555-0103  123 5th Avenue   Seattle             WA           99999   \n",
      "5  (123)555-0103  123 6th Avenue   Redmond             WA           99999   \n",
      "6  (123)555-0103  123 7th Avenue   Seattle             WA           99999   \n",
      "7  (123)555-0103  123 8th Avenue   Redmond             WA           99999   \n",
      "8  (123)555-0103  123 9th Avenue   Seattle             WA           99999   \n",
      "\n",
      "  country_region                                           web_page  \\\n",
      "0            USA                         #http://myshoptraders.com#   \n",
      "1            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "2            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "3            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "4            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "5            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "6            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "7            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "8            USA  http://myshoptraders.com#http://myshoptraders....   \n",
      "\n",
      "                                               notes attachments  \n",
      "0                                               None         b''  \n",
      "1  Joined the company as a sales representative, ...         b''  \n",
      "2  Was hired as a sales associate and was promote...         b''  \n",
      "3                                               None         b''  \n",
      "4  Joined the company as a sales representative a...         b''  \n",
      "5  Fluent in Japanese and can read and write Fren...         b''  \n",
      "6                                               None         b''  \n",
      "7                           Reads and writes French.         b''  \n",
      "8                       Fluent in French and German.         b''  \n"
     ]
    }
   ],
   "source": [
    "# Example of executing a SQL query\n",
    "my_shop_employees = \"SELECT * FROM myshop.employees;\"\n",
    "try:\n",
    "    df_query_result = db_connection.get_query_result(my_shop_employees)\n",
    "    print(\"Query Result:\")\n",
    "    print(df_query_result)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in query execution: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_dataware_house_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python to build a Multi-Source ETL Pipeline for Sales Data Analysis\n",
    "This project demonstrates the implementation of a comprehensive ETL (Extract, Transform, Load) pipeline integrating multiple data sources into a dimensional data warehouse.\n",
    "In this project, I build a data warehouse that combines sales data from MongoDB, product information from FakeStore API, and stores the transformed data in a MySQL database. The integration provides a unified view of sales transactions, product inventory, and customer information for comprehensive business analysis.\n",
    "\n",
    "### Prerequisites:\n",
    "This notebook uses the PyMongo database connectivity library to connect to MySQL databases; therefore, you must have first installed that libary into your python environment by executing the following command in a Terminal window.\n",
    "\n",
    "- `python -m pip install pymongo[srv]`\n",
    "\n",
    "#### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL Alchemy Version: 2.0.34\n",
      "Running PyMongo Version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Products Data from FakeStore API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fakestore_products():\n",
    "    \"\"\"Extract products from FakeStore API\"\"\"\n",
    "    try:\n",
    "        response = requests.get('https://fakestoreapi.com/products')\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching FakeStore products: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare & Assign Connection Variables for the MongoDB Server, the MySQL Server & Databases with which You'll be Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "# Example setup of logging for the notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to get MongoDB client\n",
    "def get_mongo_client(host: str, port: int, username: str = None, password: str = None, db_name: str = None) -> MongoClient:\n",
    "    \"\"\"Initialize MongoDB client for a remote database.\"\"\"\n",
    "    connection_string = f\"mongodb://{host}\"\n",
    "    if username and password:\n",
    "        connection_string = f\"mongodb://{username}:{password}@{host}\"\n",
    "\n",
    "    try:\n",
    "        client = MongoClient(connection_string)\n",
    "        if db_name:\n",
    "            client = client[db_name]\n",
    "        logger.info(\"MongoDB client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize MongoDB client: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# SQL connection\n",
    "def get_sql_connection(host: str, user: str, password: str, db: str):\n",
    "    \"\"\"Initialize SQL connection.\"\"\"\n",
    "    conn = pymysql.connect(host=host, user=user, password=password, db=db)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data from all sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Data directory set to: /Users/mac/Downloads/data-warehouse-project/data\n"
     ]
    }
   ],
   "source": [
    "# Set the path of the current working directory and append 'data' directory\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "logger.info(f\"Data directory set to: {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define JSON files for MongoDB collections\n",
    "json_files = {\n",
    "    \"sales_orders\": 'StoreSales.json',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mongo_collections(client: MongoClient, db_name: str, data_dir: str, json_files: dict):\n",
    "    \"\"\"Load JSON data into MongoDB collections.\"\"\"\n",
    "    db = client[db_name]\n",
    "    for collection_name, file_name in json_files.items():\n",
    "        file_path = os.path.abspath(os.path.join(data_dir, file_name))\n",
    "        \n",
    "        # Load JSON data and insert into MongoDB\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list): \n",
    "                    db[collection_name].insert_many(data)\n",
    "                    logger.info(f\"Inserted {len(data)} documents into '{collection_name}' collection.\")\n",
    "                else:\n",
    "                    db[collection_name].insert_one(data)\n",
    "                    logger.info(f\"Inserted a single document into '{collection_name}' collection.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data for collection '{collection_name}': {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection arguments (example)\n",
    "mongodb_args = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 27017,\n",
    "    \"username\": \"mikelangelo1\",\n",
    "    \"password\": \"password123\",\n",
    "    \"db_name\": \"data_ware_house\"\n",
    "}\n",
    "\n",
    "# Initialize the MongoDB client\n",
    "client = get_mongo_client(\n",
    "    host=mongodb_args[\"host\"],\n",
    "    port=mongodb_args[\"port\"],\n",
    "    username=mongodb_args.get(\"username\"),\n",
    "    password=mongodb_args.get(\"password\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate MongoDB with Source Data\n",
    "You only need to run this cell once; however, the operation is *idempotent*.  In other words, it can be run multiple times without changing the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Inserted 51291 documents into 'sales_orders' collection.\n"
     ]
    }
   ],
   "source": [
    "# Load data into MongoDB collections\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extractor\n",
    "This class provides mock methods to:\n",
    "\n",
    "Extract data from a MongoDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB client initialized successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongo_db Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'data_ware_house')\n",
      "MongoDB Data:\n",
      "                             _id Row ID         Order ID  Order Date  \\\n",
      "0       673167cbbdc428b0f7839e83  32298   CA-2012-124891  31-07-2012   \n",
      "1       673167cbbdc428b0f7839e84  26341    IN-2013-77878  05-02-2013   \n",
      "2       673167cbbdc428b0f7839e85  25330    IN-2013-71249  17-10-2013   \n",
      "3       673167cbbdc428b0f7839e86  13524  ES-2013-1579342  28-01-2013   \n",
      "4       673167cbbdc428b0f7839e87  47221     SG-2013-4320  05-11-2013   \n",
      "...                          ...    ...              ...         ...   \n",
      "307741  6751df929f0da4b4773d748a  35398   US-2014-102288  20-06-2014   \n",
      "307742  6751df929f0da4b4773d748b  40470   US-2013-155768  02-12-2013   \n",
      "307743  6751df929f0da4b4773d748c   9596   MX-2012-140767  18-02-2012   \n",
      "307744  6751df929f0da4b4773d748d   6147   MX-2012-134460  22-05-2012   \n",
      "307745  6751df929f0da4b4773d748e                     NaN         NaN   \n",
      "\n",
      "         Ship Date       Ship Mode Customer ID     Customer Name      Segment  \\\n",
      "0       31-07-2012        Same Day    RH-19495       Rick Hansen     Consumer   \n",
      "1       07-02-2013    Second Class    JR-16210     Justin Ritter    Corporate   \n",
      "2       18-10-2013     First Class    CR-12730      Craig Reiter     Consumer   \n",
      "3       30-01-2013     First Class    KM-16375  Katherine Murray  Home Office   \n",
      "4       06-11-2013        Same Day     RH-9495       Rick Hansen     Consumer   \n",
      "...            ...             ...         ...               ...          ...   \n",
      "307741  24-06-2014  Standard Class    ZC-21910  Zuschuss Carroll     Consumer   \n",
      "307742  02-12-2013        Same Day    LB-16795    Laurel Beltran  Home Office   \n",
      "307743  22-02-2012  Standard Class    RB-19795        Ross Baird  Home Office   \n",
      "307744  26-05-2012    Second Class    MC-18100     Mick Crebagga     Consumer   \n",
      "307745         NaN             NaN         NaN               NaN          NaN   \n",
      "\n",
      "                 City  ...        Product ID         Category Sub-Category  \\\n",
      "0       New York City  ...   TEC-AC-10003033       Technology  Accessories   \n",
      "1          Wollongong  ...   FUR-CH-10003950        Furniture       Chairs   \n",
      "2            Brisbane  ...   TEC-PH-10004664       Technology       Phones   \n",
      "3              Berlin  ...   TEC-PH-10004583       Technology       Phones   \n",
      "4               Dakar  ...  TEC-SHA-10000501       Technology      Copiers   \n",
      "...               ...  ...               ...              ...          ...   \n",
      "307741        Houston  ...   OFF-AP-10002906  Office Supplies   Appliances   \n",
      "307742         Oxnard  ...   OFF-EN-10001219  Office Supplies    Envelopes   \n",
      "307743       Valinhos  ...   OFF-BI-10000806  Office Supplies      Binders   \n",
      "307744       Tipitapa  ...   OFF-PA-10004155  Office Supplies        Paper   \n",
      "307745            NaN  ...               NaN              NaN          NaN   \n",
      "\n",
      "                                             Product Name     Sales Quantity  \\\n",
      "0       Plantronics CS510 - Over-the-Head monaural Wir...   2309.65        7   \n",
      "1               Novimex Executive Leather Armchair, Black  3709.395        9   \n",
      "2                       Nokia Smart Phone, with Caller ID  5175.171        9   \n",
      "3                          Motorola Smart Phone, Cordless   2892.51        5   \n",
      "4                          Sharp Wireless Fax, High-Speed   2832.96        8   \n",
      "...                                                   ...       ...      ...   \n",
      "307741  Hoover Replacement Belt for Commercial Guardsm...     0.444        1   \n",
      "307742       #10- 4 1/8\" x 9 1/2\" Security-Tint Envelopes     22.92        3   \n",
      "307743                            Acco Index Tab, Economy     13.44        2   \n",
      "307744            Eaton Computer Printout Paper, 8.5 x 11     61.38        3   \n",
      "307745                                                NaN       NaN      NaN   \n",
      "\n",
      "       Discount    Profit Shipping Cost Order Priority  \n",
      "0             0  762.1845        933.57       Critical  \n",
      "1           0.1  -288.765        923.63       Critical  \n",
      "2           0.1   919.971        915.49         Medium  \n",
      "3           0.1    -96.54        910.16         Medium  \n",
      "4             0    311.52        903.04       Critical  \n",
      "...         ...       ...           ...            ...  \n",
      "307741      0.8     -1.11          0.01         Medium  \n",
      "307742        0   11.2308          0.01           High  \n",
      "307743        0       2.4             0         Medium  \n",
      "307744        0       1.8             0           High  \n",
      "307745      NaN       NaN           NaN            NaN  \n",
      "\n",
      "[307746 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracted 20 products from FakeStore API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api Data:     id                                              title   price  \\\n",
      "0    1  Fjallraven - Foldsack No. 1 Backpack, Fits 15 ...  109.95   \n",
      "1    2             Mens Casual Premium Slim Fit T-Shirts    22.30   \n",
      "2    3                                 Mens Cotton Jacket   55.99   \n",
      "3    4                               Mens Casual Slim Fit   15.99   \n",
      "4    5  John Hardy Women's Legends Naga Gold & Silver ...  695.00   \n",
      "5    6                       Solid Gold Petite Micropave   168.00   \n",
      "6    7                         White Gold Plated Princess    9.99   \n",
      "7    8  Pierced Owl Rose Gold Plated Stainless Steel D...   10.99   \n",
      "8    9  WD 2TB Elements Portable External Hard Drive -...   64.00   \n",
      "9   10  SanDisk SSD PLUS 1TB Internal SSD - SATA III 6...  109.00   \n",
      "10  11  Silicon Power 256GB SSD 3D NAND A55 SLC Cache ...  109.00   \n",
      "11  12  WD 4TB Gaming Drive Works with Playstation 4 P...  114.00   \n",
      "12  13  Acer SB220Q bi 21.5 inches Full HD (1920 x 108...  599.00   \n",
      "13  14  Samsung 49-Inch CHG90 144Hz Curved Gaming Moni...  999.99   \n",
      "14  15  BIYLACLESEN Women's 3-in-1 Snowboard Jacket Wi...   56.99   \n",
      "15  16  Lock and Love Women's Removable Hooded Faux Le...   29.95   \n",
      "16  17  Rain Jacket Women Windbreaker Striped Climbing...   39.99   \n",
      "17  18        MBJ Women's Solid Short Sleeve Boat Neck V     9.85   \n",
      "18  19                 Opna Women's Short Sleeve Moisture    7.95   \n",
      "19  20         DANVOUY Womens T Shirt Casual Cotton Short   12.99   \n",
      "\n",
      "                                          description          category  \\\n",
      "0   Your perfect pack for everyday use and walks i...    men's clothing   \n",
      "1   Slim-fitting style, contrast raglan long sleev...    men's clothing   \n",
      "2   great outerwear jackets for Spring/Autumn/Wint...    men's clothing   \n",
      "3   The color could be slightly different between ...    men's clothing   \n",
      "4   From our Legends Collection, the Naga was insp...          jewelery   \n",
      "5   Satisfaction Guaranteed. Return or exchange an...          jewelery   \n",
      "6   Classic Created Wedding Engagement Solitaire D...          jewelery   \n",
      "7   Rose Gold Plated Double Flared Tunnel Plug Ear...          jewelery   \n",
      "8   USB 3.0 and USB 2.0 Compatibility Fast data tr...       electronics   \n",
      "9   Easy upgrade for faster boot up, shutdown, app...       electronics   \n",
      "10  3D NAND flash are applied to deliver high tran...       electronics   \n",
      "11  Expand your PS4 gaming experience, Play anywhe...       electronics   \n",
      "12  21. 5 inches Full HD (1920 x 1080) widescreen ...       electronics   \n",
      "13  49 INCH SUPER ULTRAWIDE 32:9 CURVED GAMING MON...       electronics   \n",
      "14  Note:The Jackets is US standard size, Please c...  women's clothing   \n",
      "15  100% POLYURETHANE(shell) 100% POLYESTER(lining...  women's clothing   \n",
      "16  Lightweight perfet for trip or casual wear---L...  women's clothing   \n",
      "17  95% RAYON 5% SPANDEX, Made in USA or Imported,...  women's clothing   \n",
      "18  100% Polyester, Machine wash, 100% cationic po...  women's clothing   \n",
      "19  95%Cotton,5%Spandex, Features: Casual, Short S...  women's clothing   \n",
      "\n",
      "                                                image  \\\n",
      "0   https://fakestoreapi.com/img/81fPKd-2AYL._AC_S...   \n",
      "1   https://fakestoreapi.com/img/71-3HjGNDUL._AC_S...   \n",
      "2   https://fakestoreapi.com/img/71li-ujtlUL._AC_U...   \n",
      "3   https://fakestoreapi.com/img/71YXzeOuslL._AC_U...   \n",
      "4   https://fakestoreapi.com/img/71pWzhdJNwL._AC_U...   \n",
      "5   https://fakestoreapi.com/img/61sbMiUnoGL._AC_U...   \n",
      "6   https://fakestoreapi.com/img/71YAIFU48IL._AC_U...   \n",
      "7   https://fakestoreapi.com/img/51UDEzMJVpL._AC_U...   \n",
      "8   https://fakestoreapi.com/img/61IBBVJvSDL._AC_S...   \n",
      "9   https://fakestoreapi.com/img/61U7T1koQqL._AC_S...   \n",
      "10  https://fakestoreapi.com/img/71kWymZ+c+L._AC_S...   \n",
      "11  https://fakestoreapi.com/img/61mtL65D4cL._AC_S...   \n",
      "12  https://fakestoreapi.com/img/81QpkIctqPL._AC_S...   \n",
      "13  https://fakestoreapi.com/img/81Zt42ioCgL._AC_S...   \n",
      "14  https://fakestoreapi.com/img/51Y5NI-I5jL._AC_U...   \n",
      "15  https://fakestoreapi.com/img/81XH0e8fefL._AC_U...   \n",
      "16  https://fakestoreapi.com/img/71HblAHs5xL._AC_U...   \n",
      "17  https://fakestoreapi.com/img/71z3kpMAYsL._AC_U...   \n",
      "18  https://fakestoreapi.com/img/51eg55uWmdL._AC_U...   \n",
      "19  https://fakestoreapi.com/img/61pHAEJ4NML._AC_U...   \n",
      "\n",
      "                         rating  \n",
      "0   {'rate': 3.9, 'count': 120}  \n",
      "1   {'rate': 4.1, 'count': 259}  \n",
      "2   {'rate': 4.7, 'count': 500}  \n",
      "3   {'rate': 2.1, 'count': 430}  \n",
      "4   {'rate': 4.6, 'count': 400}  \n",
      "5    {'rate': 3.9, 'count': 70}  \n",
      "6     {'rate': 3, 'count': 400}  \n",
      "7   {'rate': 1.9, 'count': 100}  \n",
      "8   {'rate': 3.3, 'count': 203}  \n",
      "9   {'rate': 2.9, 'count': 470}  \n",
      "10  {'rate': 4.8, 'count': 319}  \n",
      "11  {'rate': 4.8, 'count': 400}  \n",
      "12  {'rate': 2.9, 'count': 250}  \n",
      "13  {'rate': 2.2, 'count': 140}  \n",
      "14  {'rate': 2.6, 'count': 235}  \n",
      "15  {'rate': 2.9, 'count': 340}  \n",
      "16  {'rate': 3.8, 'count': 679}  \n",
      "17  {'rate': 4.7, 'count': 130}  \n",
      "18  {'rate': 4.5, 'count': 146}  \n",
      "19  {'rate': 3.6, 'count': 145}  \n",
      "                             _id Row ID         Order ID  Order Date  \\\n",
      "0       673167cbbdc428b0f7839e83  32298   CA-2012-124891  31-07-2012   \n",
      "1       673167cbbdc428b0f7839e84  26341    IN-2013-77878  05-02-2013   \n",
      "2       673167cbbdc428b0f7839e85  25330    IN-2013-71249  17-10-2013   \n",
      "3       673167cbbdc428b0f7839e86  13524  ES-2013-1579342  28-01-2013   \n",
      "4       673167cbbdc428b0f7839e87  47221     SG-2013-4320  05-11-2013   \n",
      "...                          ...    ...              ...         ...   \n",
      "307741  6751df929f0da4b4773d748a  35398   US-2014-102288  20-06-2014   \n",
      "307742  6751df929f0da4b4773d748b  40470   US-2013-155768  02-12-2013   \n",
      "307743  6751df929f0da4b4773d748c   9596   MX-2012-140767  18-02-2012   \n",
      "307744  6751df929f0da4b4773d748d   6147   MX-2012-134460  22-05-2012   \n",
      "307745  6751df929f0da4b4773d748e                     NaN         NaN   \n",
      "\n",
      "         Ship Date       Ship Mode Customer ID     Customer Name      Segment  \\\n",
      "0       31-07-2012        Same Day    RH-19495       Rick Hansen     Consumer   \n",
      "1       07-02-2013    Second Class    JR-16210     Justin Ritter    Corporate   \n",
      "2       18-10-2013     First Class    CR-12730      Craig Reiter     Consumer   \n",
      "3       30-01-2013     First Class    KM-16375  Katherine Murray  Home Office   \n",
      "4       06-11-2013        Same Day     RH-9495       Rick Hansen     Consumer   \n",
      "...            ...             ...         ...               ...          ...   \n",
      "307741  24-06-2014  Standard Class    ZC-21910  Zuschuss Carroll     Consumer   \n",
      "307742  02-12-2013        Same Day    LB-16795    Laurel Beltran  Home Office   \n",
      "307743  22-02-2012  Standard Class    RB-19795        Ross Baird  Home Office   \n",
      "307744  26-05-2012    Second Class    MC-18100     Mick Crebagga     Consumer   \n",
      "307745         NaN             NaN         NaN               NaN          NaN   \n",
      "\n",
      "                 City  ...        Product ID         Category Sub-Category  \\\n",
      "0       New York City  ...   TEC-AC-10003033       Technology  Accessories   \n",
      "1          Wollongong  ...   FUR-CH-10003950        Furniture       Chairs   \n",
      "2            Brisbane  ...   TEC-PH-10004664       Technology       Phones   \n",
      "3              Berlin  ...   TEC-PH-10004583       Technology       Phones   \n",
      "4               Dakar  ...  TEC-SHA-10000501       Technology      Copiers   \n",
      "...               ...  ...               ...              ...          ...   \n",
      "307741        Houston  ...   OFF-AP-10002906  Office Supplies   Appliances   \n",
      "307742         Oxnard  ...   OFF-EN-10001219  Office Supplies    Envelopes   \n",
      "307743       Valinhos  ...   OFF-BI-10000806  Office Supplies      Binders   \n",
      "307744       Tipitapa  ...   OFF-PA-10004155  Office Supplies        Paper   \n",
      "307745            NaN  ...               NaN              NaN          NaN   \n",
      "\n",
      "                                             Product Name     Sales Quantity  \\\n",
      "0       Plantronics CS510 - Over-the-Head monaural Wir...   2309.65        7   \n",
      "1               Novimex Executive Leather Armchair, Black  3709.395        9   \n",
      "2                       Nokia Smart Phone, with Caller ID  5175.171        9   \n",
      "3                          Motorola Smart Phone, Cordless   2892.51        5   \n",
      "4                          Sharp Wireless Fax, High-Speed   2832.96        8   \n",
      "...                                                   ...       ...      ...   \n",
      "307741  Hoover Replacement Belt for Commercial Guardsm...     0.444        1   \n",
      "307742       #10- 4 1/8\" x 9 1/2\" Security-Tint Envelopes     22.92        3   \n",
      "307743                            Acco Index Tab, Economy     13.44        2   \n",
      "307744            Eaton Computer Printout Paper, 8.5 x 11     61.38        3   \n",
      "307745                                                NaN       NaN      NaN   \n",
      "\n",
      "       Discount    Profit Shipping Cost Order Priority  \n",
      "0             0  762.1845        933.57       Critical  \n",
      "1           0.1  -288.765        923.63       Critical  \n",
      "2           0.1   919.971        915.49         Medium  \n",
      "3           0.1    -96.54        910.16         Medium  \n",
      "4             0    311.52        903.04       Critical  \n",
      "...         ...       ...           ...            ...  \n",
      "307741      0.8     -1.11          0.01         Medium  \n",
      "307742        0   11.2308          0.01           High  \n",
      "307743        0       2.4             0         Medium  \n",
      "307744        0       1.8             0           High  \n",
      "307745      NaN       NaN           NaN            NaN  \n",
      "\n",
      "[307746 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_from_mongodb(collection: str, query: Dict = None) -> pd.DataFrame:\n",
    "    \"\"\"Extract data from MongoDB collection.\"\"\"\n",
    "    try:\n",
    "        mongo_db = get_mongo_client(mongodb_args[\"host\"], mongodb_args[\"port\"], mongodb_args[\"username\"], mongodb_args[\"password\"], mongodb_args[\"db_name\"])\n",
    "        print(\"mongo_db\", mongo_db)\n",
    "        data = mongo_db[collection].find(query or {})  \n",
    "        return pd.DataFrame(list(data))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting from MongoDB collection '{collection}': {str(e)}\")\n",
    "        raise     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB client initialized successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongo_db Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'data_ware_house')\n",
      "MongoDB Data:\n",
      "                             _id Row ID         Order ID  Order Date  \\\n",
      "0       673167cbbdc428b0f7839e83  32298   CA-2012-124891  31-07-2012   \n",
      "1       673167cbbdc428b0f7839e84  26341    IN-2013-77878  05-02-2013   \n",
      "2       673167cbbdc428b0f7839e85  25330    IN-2013-71249  17-10-2013   \n",
      "3       673167cbbdc428b0f7839e86  13524  ES-2013-1579342  28-01-2013   \n",
      "4       673167cbbdc428b0f7839e87  47221     SG-2013-4320  05-11-2013   \n",
      "...                          ...    ...              ...         ...   \n",
      "307741  6751df929f0da4b4773d748a  35398   US-2014-102288  20-06-2014   \n",
      "307742  6751df929f0da4b4773d748b  40470   US-2013-155768  02-12-2013   \n",
      "307743  6751df929f0da4b4773d748c   9596   MX-2012-140767  18-02-2012   \n",
      "307744  6751df929f0da4b4773d748d   6147   MX-2012-134460  22-05-2012   \n",
      "307745  6751df929f0da4b4773d748e                     NaN         NaN   \n",
      "\n",
      "         Ship Date       Ship Mode Customer ID     Customer Name      Segment  \\\n",
      "0       31-07-2012        Same Day    RH-19495       Rick Hansen     Consumer   \n",
      "1       07-02-2013    Second Class    JR-16210     Justin Ritter    Corporate   \n",
      "2       18-10-2013     First Class    CR-12730      Craig Reiter     Consumer   \n",
      "3       30-01-2013     First Class    KM-16375  Katherine Murray  Home Office   \n",
      "4       06-11-2013        Same Day     RH-9495       Rick Hansen     Consumer   \n",
      "...            ...             ...         ...               ...          ...   \n",
      "307741  24-06-2014  Standard Class    ZC-21910  Zuschuss Carroll     Consumer   \n",
      "307742  02-12-2013        Same Day    LB-16795    Laurel Beltran  Home Office   \n",
      "307743  22-02-2012  Standard Class    RB-19795        Ross Baird  Home Office   \n",
      "307744  26-05-2012    Second Class    MC-18100     Mick Crebagga     Consumer   \n",
      "307745         NaN             NaN         NaN               NaN          NaN   \n",
      "\n",
      "                 City  ...        Product ID         Category Sub-Category  \\\n",
      "0       New York City  ...   TEC-AC-10003033       Technology  Accessories   \n",
      "1          Wollongong  ...   FUR-CH-10003950        Furniture       Chairs   \n",
      "2            Brisbane  ...   TEC-PH-10004664       Technology       Phones   \n",
      "3              Berlin  ...   TEC-PH-10004583       Technology       Phones   \n",
      "4               Dakar  ...  TEC-SHA-10000501       Technology      Copiers   \n",
      "...               ...  ...               ...              ...          ...   \n",
      "307741        Houston  ...   OFF-AP-10002906  Office Supplies   Appliances   \n",
      "307742         Oxnard  ...   OFF-EN-10001219  Office Supplies    Envelopes   \n",
      "307743       Valinhos  ...   OFF-BI-10000806  Office Supplies      Binders   \n",
      "307744       Tipitapa  ...   OFF-PA-10004155  Office Supplies        Paper   \n",
      "307745            NaN  ...               NaN              NaN          NaN   \n",
      "\n",
      "                                             Product Name     Sales Quantity  \\\n",
      "0       Plantronics CS510 - Over-the-Head monaural Wir...   2309.65        7   \n",
      "1               Novimex Executive Leather Armchair, Black  3709.395        9   \n",
      "2                       Nokia Smart Phone, with Caller ID  5175.171        9   \n",
      "3                          Motorola Smart Phone, Cordless   2892.51        5   \n",
      "4                          Sharp Wireless Fax, High-Speed   2832.96        8   \n",
      "...                                                   ...       ...      ...   \n",
      "307741  Hoover Replacement Belt for Commercial Guardsm...     0.444        1   \n",
      "307742       #10- 4 1/8\" x 9 1/2\" Security-Tint Envelopes     22.92        3   \n",
      "307743                            Acco Index Tab, Economy     13.44        2   \n",
      "307744            Eaton Computer Printout Paper, 8.5 x 11     61.38        3   \n",
      "307745                                                NaN       NaN      NaN   \n",
      "\n",
      "       Discount    Profit Shipping Cost Order Priority  \n",
      "0             0  762.1845        933.57       Critical  \n",
      "1           0.1  -288.765        923.63       Critical  \n",
      "2           0.1   919.971        915.49         Medium  \n",
      "3           0.1    -96.54        910.16         Medium  \n",
      "4             0    311.52        903.04       Critical  \n",
      "...         ...       ...           ...            ...  \n",
      "307741      0.8     -1.11          0.01         Medium  \n",
      "307742        0   11.2308          0.01           High  \n",
      "307743        0       2.4             0         Medium  \n",
      "307744        0       1.8             0           High  \n",
      "307745      NaN       NaN           NaN            NaN  \n",
      "\n",
      "[307746 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracted 20 products from FakeStore API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api Data:     id                                              title   price  \\\n",
      "0    1  Fjallraven - Foldsack No. 1 Backpack, Fits 15 ...  109.95   \n",
      "1    2             Mens Casual Premium Slim Fit T-Shirts    22.30   \n",
      "2    3                                 Mens Cotton Jacket   55.99   \n",
      "3    4                               Mens Casual Slim Fit   15.99   \n",
      "4    5  John Hardy Women's Legends Naga Gold & Silver ...  695.00   \n",
      "5    6                       Solid Gold Petite Micropave   168.00   \n",
      "6    7                         White Gold Plated Princess    9.99   \n",
      "7    8  Pierced Owl Rose Gold Plated Stainless Steel D...   10.99   \n",
      "8    9  WD 2TB Elements Portable External Hard Drive -...   64.00   \n",
      "9   10  SanDisk SSD PLUS 1TB Internal SSD - SATA III 6...  109.00   \n",
      "10  11  Silicon Power 256GB SSD 3D NAND A55 SLC Cache ...  109.00   \n",
      "11  12  WD 4TB Gaming Drive Works with Playstation 4 P...  114.00   \n",
      "12  13  Acer SB220Q bi 21.5 inches Full HD (1920 x 108...  599.00   \n",
      "13  14  Samsung 49-Inch CHG90 144Hz Curved Gaming Moni...  999.99   \n",
      "14  15  BIYLACLESEN Women's 3-in-1 Snowboard Jacket Wi...   56.99   \n",
      "15  16  Lock and Love Women's Removable Hooded Faux Le...   29.95   \n",
      "16  17  Rain Jacket Women Windbreaker Striped Climbing...   39.99   \n",
      "17  18        MBJ Women's Solid Short Sleeve Boat Neck V     9.85   \n",
      "18  19                 Opna Women's Short Sleeve Moisture    7.95   \n",
      "19  20         DANVOUY Womens T Shirt Casual Cotton Short   12.99   \n",
      "\n",
      "                                          description          category  \\\n",
      "0   Your perfect pack for everyday use and walks i...    men's clothing   \n",
      "1   Slim-fitting style, contrast raglan long sleev...    men's clothing   \n",
      "2   great outerwear jackets for Spring/Autumn/Wint...    men's clothing   \n",
      "3   The color could be slightly different between ...    men's clothing   \n",
      "4   From our Legends Collection, the Naga was insp...          jewelery   \n",
      "5   Satisfaction Guaranteed. Return or exchange an...          jewelery   \n",
      "6   Classic Created Wedding Engagement Solitaire D...          jewelery   \n",
      "7   Rose Gold Plated Double Flared Tunnel Plug Ear...          jewelery   \n",
      "8   USB 3.0 and USB 2.0 Compatibility Fast data tr...       electronics   \n",
      "9   Easy upgrade for faster boot up, shutdown, app...       electronics   \n",
      "10  3D NAND flash are applied to deliver high tran...       electronics   \n",
      "11  Expand your PS4 gaming experience, Play anywhe...       electronics   \n",
      "12  21. 5 inches Full HD (1920 x 1080) widescreen ...       electronics   \n",
      "13  49 INCH SUPER ULTRAWIDE 32:9 CURVED GAMING MON...       electronics   \n",
      "14  Note:The Jackets is US standard size, Please c...  women's clothing   \n",
      "15  100% POLYURETHANE(shell) 100% POLYESTER(lining...  women's clothing   \n",
      "16  Lightweight perfet for trip or casual wear---L...  women's clothing   \n",
      "17  95% RAYON 5% SPANDEX, Made in USA or Imported,...  women's clothing   \n",
      "18  100% Polyester, Machine wash, 100% cationic po...  women's clothing   \n",
      "19  95%Cotton,5%Spandex, Features: Casual, Short S...  women's clothing   \n",
      "\n",
      "                                                image  \\\n",
      "0   https://fakestoreapi.com/img/81fPKd-2AYL._AC_S...   \n",
      "1   https://fakestoreapi.com/img/71-3HjGNDUL._AC_S...   \n",
      "2   https://fakestoreapi.com/img/71li-ujtlUL._AC_U...   \n",
      "3   https://fakestoreapi.com/img/71YXzeOuslL._AC_U...   \n",
      "4   https://fakestoreapi.com/img/71pWzhdJNwL._AC_U...   \n",
      "5   https://fakestoreapi.com/img/61sbMiUnoGL._AC_U...   \n",
      "6   https://fakestoreapi.com/img/71YAIFU48IL._AC_U...   \n",
      "7   https://fakestoreapi.com/img/51UDEzMJVpL._AC_U...   \n",
      "8   https://fakestoreapi.com/img/61IBBVJvSDL._AC_S...   \n",
      "9   https://fakestoreapi.com/img/61U7T1koQqL._AC_S...   \n",
      "10  https://fakestoreapi.com/img/71kWymZ+c+L._AC_S...   \n",
      "11  https://fakestoreapi.com/img/61mtL65D4cL._AC_S...   \n",
      "12  https://fakestoreapi.com/img/81QpkIctqPL._AC_S...   \n",
      "13  https://fakestoreapi.com/img/81Zt42ioCgL._AC_S...   \n",
      "14  https://fakestoreapi.com/img/51Y5NI-I5jL._AC_U...   \n",
      "15  https://fakestoreapi.com/img/81XH0e8fefL._AC_U...   \n",
      "16  https://fakestoreapi.com/img/71HblAHs5xL._AC_U...   \n",
      "17  https://fakestoreapi.com/img/71z3kpMAYsL._AC_U...   \n",
      "18  https://fakestoreapi.com/img/51eg55uWmdL._AC_U...   \n",
      "19  https://fakestoreapi.com/img/61pHAEJ4NML._AC_U...   \n",
      "\n",
      "                         rating  \n",
      "0   {'rate': 3.9, 'count': 120}  \n",
      "1   {'rate': 4.1, 'count': 259}  \n",
      "2   {'rate': 4.7, 'count': 500}  \n",
      "3   {'rate': 2.1, 'count': 430}  \n",
      "4   {'rate': 4.6, 'count': 400}  \n",
      "5    {'rate': 3.9, 'count': 70}  \n",
      "6     {'rate': 3, 'count': 400}  \n",
      "7   {'rate': 1.9, 'count': 100}  \n",
      "8   {'rate': 3.3, 'count': 203}  \n",
      "9   {'rate': 2.9, 'count': 470}  \n",
      "10  {'rate': 4.8, 'count': 319}  \n",
      "11  {'rate': 4.8, 'count': 400}  \n",
      "12  {'rate': 2.9, 'count': 250}  \n",
      "13  {'rate': 2.2, 'count': 140}  \n",
      "14  {'rate': 2.6, 'count': 235}  \n",
      "15  {'rate': 2.9, 'count': 340}  \n",
      "16  {'rate': 3.8, 'count': 679}  \n",
      "17  {'rate': 4.7, 'count': 130}  \n",
      "18  {'rate': 4.5, 'count': 146}  \n",
      "19  {'rate': 3.6, 'count': 145}  \n",
      "                             _id Row ID         Order ID  Order Date  \\\n",
      "0       673167cbbdc428b0f7839e83  32298   CA-2012-124891  31-07-2012   \n",
      "1       673167cbbdc428b0f7839e84  26341    IN-2013-77878  05-02-2013   \n",
      "2       673167cbbdc428b0f7839e85  25330    IN-2013-71249  17-10-2013   \n",
      "3       673167cbbdc428b0f7839e86  13524  ES-2013-1579342  28-01-2013   \n",
      "4       673167cbbdc428b0f7839e87  47221     SG-2013-4320  05-11-2013   \n",
      "...                          ...    ...              ...         ...   \n",
      "307741  6751df929f0da4b4773d748a  35398   US-2014-102288  20-06-2014   \n",
      "307742  6751df929f0da4b4773d748b  40470   US-2013-155768  02-12-2013   \n",
      "307743  6751df929f0da4b4773d748c   9596   MX-2012-140767  18-02-2012   \n",
      "307744  6751df929f0da4b4773d748d   6147   MX-2012-134460  22-05-2012   \n",
      "307745  6751df929f0da4b4773d748e                     NaN         NaN   \n",
      "\n",
      "         Ship Date       Ship Mode Customer ID     Customer Name      Segment  \\\n",
      "0       31-07-2012        Same Day    RH-19495       Rick Hansen     Consumer   \n",
      "1       07-02-2013    Second Class    JR-16210     Justin Ritter    Corporate   \n",
      "2       18-10-2013     First Class    CR-12730      Craig Reiter     Consumer   \n",
      "3       30-01-2013     First Class    KM-16375  Katherine Murray  Home Office   \n",
      "4       06-11-2013        Same Day     RH-9495       Rick Hansen     Consumer   \n",
      "...            ...             ...         ...               ...          ...   \n",
      "307741  24-06-2014  Standard Class    ZC-21910  Zuschuss Carroll     Consumer   \n",
      "307742  02-12-2013        Same Day    LB-16795    Laurel Beltran  Home Office   \n",
      "307743  22-02-2012  Standard Class    RB-19795        Ross Baird  Home Office   \n",
      "307744  26-05-2012    Second Class    MC-18100     Mick Crebagga     Consumer   \n",
      "307745         NaN             NaN         NaN               NaN          NaN   \n",
      "\n",
      "                 City  ...        Product ID         Category Sub-Category  \\\n",
      "0       New York City  ...   TEC-AC-10003033       Technology  Accessories   \n",
      "1          Wollongong  ...   FUR-CH-10003950        Furniture       Chairs   \n",
      "2            Brisbane  ...   TEC-PH-10004664       Technology       Phones   \n",
      "3              Berlin  ...   TEC-PH-10004583       Technology       Phones   \n",
      "4               Dakar  ...  TEC-SHA-10000501       Technology      Copiers   \n",
      "...               ...  ...               ...              ...          ...   \n",
      "307741        Houston  ...   OFF-AP-10002906  Office Supplies   Appliances   \n",
      "307742         Oxnard  ...   OFF-EN-10001219  Office Supplies    Envelopes   \n",
      "307743       Valinhos  ...   OFF-BI-10000806  Office Supplies      Binders   \n",
      "307744       Tipitapa  ...   OFF-PA-10004155  Office Supplies        Paper   \n",
      "307745            NaN  ...               NaN              NaN          NaN   \n",
      "\n",
      "                                             Product Name     Sales Quantity  \\\n",
      "0       Plantronics CS510 - Over-the-Head monaural Wir...   2309.65        7   \n",
      "1               Novimex Executive Leather Armchair, Black  3709.395        9   \n",
      "2                       Nokia Smart Phone, with Caller ID  5175.171        9   \n",
      "3                          Motorola Smart Phone, Cordless   2892.51        5   \n",
      "4                          Sharp Wireless Fax, High-Speed   2832.96        8   \n",
      "...                                                   ...       ...      ...   \n",
      "307741  Hoover Replacement Belt for Commercial Guardsm...     0.444        1   \n",
      "307742       #10- 4 1/8\" x 9 1/2\" Security-Tint Envelopes     22.92        3   \n",
      "307743                            Acco Index Tab, Economy     13.44        2   \n",
      "307744            Eaton Computer Printout Paper, 8.5 x 11     61.38        3   \n",
      "307745                                                NaN       NaN      NaN   \n",
      "\n",
      "       Discount    Profit Shipping Cost Order Priority  \n",
      "0             0  762.1845        933.57       Critical  \n",
      "1           0.1  -288.765        923.63       Critical  \n",
      "2           0.1   919.971        915.49         Medium  \n",
      "3           0.1    -96.54        910.16         Medium  \n",
      "4             0    311.52        903.04       Critical  \n",
      "...         ...       ...           ...            ...  \n",
      "307741      0.8     -1.11          0.01         Medium  \n",
      "307742        0   11.2308          0.01           High  \n",
      "307743        0       2.4             0         Medium  \n",
      "307744        0       1.8             0           High  \n",
      "307745      NaN       NaN           NaN            NaN  \n",
      "\n",
      "[307746 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Transforming data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date_id       date  year  quarter  month month_name  week  day  \\\n",
      "2011-01-01  20110101 2011-01-01  2011        1      1    January    52    1   \n",
      "2011-01-02  20110102 2011-01-02  2011        1      1    January    52    2   \n",
      "2011-01-03  20110103 2011-01-03  2011        1      1    January     1    3   \n",
      "2011-01-04  20110104 2011-01-04  2011        1      1    January     1    4   \n",
      "2011-01-05  20110105 2011-01-05  2011        1      1    January     1    5   \n",
      "...              ...        ...   ...      ...    ...        ...   ...  ...   \n",
      "2015-01-03  20150103 2015-01-03  2015        1      1    January     1    3   \n",
      "2015-01-04  20150104 2015-01-04  2015        1      1    January     1    4   \n",
      "2015-01-05  20150105 2015-01-05  2015        1      1    January     2    5   \n",
      "2015-01-06  20150106 2015-01-06  2015        1      1    January     2    6   \n",
      "2015-01-07  20150107 2015-01-07  2015        1      1    January     2    7   \n",
      "\n",
      "            weekday weekday_name  is_weekend  \n",
      "2011-01-01        5     Saturday           1  \n",
      "2011-01-02        6       Sunday           1  \n",
      "2011-01-03        0       Monday           0  \n",
      "2011-01-04        1      Tuesday           0  \n",
      "2011-01-05        2    Wednesday           0  \n",
      "...             ...          ...         ...  \n",
      "2015-01-03        5     Saturday           1  \n",
      "2015-01-04        6       Sunday           1  \n",
      "2015-01-05        0       Monday           0  \n",
      "2015-01-06        1      Tuesday           0  \n",
      "2015-01-07        2    Wednesday           0  \n",
      "\n",
      "[1468 rows x 11 columns]\n",
      "      customer_id      customer_name      segment               city  \\\n",
      "0        RH-19495        Rick Hansen     Consumer      New York City   \n",
      "1        JR-16210      Justin Ritter    Corporate         Wollongong   \n",
      "2        CR-12730       Craig Reiter     Consumer           Brisbane   \n",
      "3        KM-16375   Katherine Murray  Home Office             Berlin   \n",
      "4         RH-9495        Rick Hansen     Consumer              Dakar   \n",
      "...           ...                ...          ...                ...   \n",
      "51281    HG-15025      Hunter Glantz     Consumer  Bragan�a Paulista   \n",
      "51283    KH-16330    Katharine Harms    Corporate            Lucknow   \n",
      "51284    DB-13270  Deborah Brumfield  Home Office         Townsville   \n",
      "51289    MC-18100      Mick Crebagga     Consumer           Tipitapa   \n",
      "51290     UNKNOWN            Unknown      Unknown            Unknown   \n",
      "\n",
      "                 state        country   market        region  customer_key  \n",
      "0             New York  United States       US          East             1  \n",
      "1      New South Wales      Australia     APAC       Oceania             2  \n",
      "2           Queensland      Australia     APAC       Oceania             3  \n",
      "3               Berlin        Germany       EU       Central             4  \n",
      "4                Dakar        Senegal   Africa        Africa             5  \n",
      "...                ...            ...      ...           ...           ...  \n",
      "51281        S�o Paulo         Brazil    LATAM         South         24956  \n",
      "51283    Uttar Pradesh          India     APAC  Central Asia         24957  \n",
      "51284       Queensland      Australia     APAC       Oceania         24958  \n",
      "51289          Managua      Nicaragua    LATAM       Central         24959  \n",
      "51290          Unknown        Unknown  Unknown       Unknown         24960  \n",
      "\n",
      "[24960 rows x 9 columns]\n",
      "             product_id                                       product_name  \\\n",
      "0       TEC-AC-10003033  Plantronics CS510 - Over-the-Head monaural Wir...   \n",
      "1       FUR-CH-10003950          Novimex Executive Leather Armchair, Black   \n",
      "2       TEC-PH-10004664                  Nokia Smart Phone, with Caller ID   \n",
      "3       TEC-PH-10004583                     Motorola Smart Phone, Cordless   \n",
      "4      TEC-SHA-10000501                     Sharp Wireless Fax, High-Speed   \n",
      "...                 ...                                                ...   \n",
      "10784            API_16  Lock and Love Women's Removable Hooded Faux Le...   \n",
      "10785            API_17  Rain Jacket Women Windbreaker Striped Climbing...   \n",
      "10786            API_18        MBJ Women's Solid Short Sleeve Boat Neck V    \n",
      "10787            API_19                 Opna Women's Short Sleeve Moisture   \n",
      "10788            API_20         DANVOUY Womens T Shirt Casual Cotton Short   \n",
      "\n",
      "               category  subcategory source  product_key  \n",
      "0            Technology  Accessories  sales            1  \n",
      "1             Furniture       Chairs  sales            2  \n",
      "2            Technology       Phones  sales            3  \n",
      "3            Technology       Phones  sales            4  \n",
      "4            Technology      Copiers  sales            5  \n",
      "...                 ...          ...    ...          ...  \n",
      "10784  women's clothing          N/A    api        10785  \n",
      "10785  women's clothing          N/A    api        10786  \n",
      "10786  women's clothing          N/A    api        10787  \n",
      "10787  women's clothing          N/A    api        10788  \n",
      "10788  women's clothing          N/A    api        10789  \n",
      "\n",
      "[10789 rows x 6 columns]\n",
      "                 city               state        country   market   region  \\\n",
      "0       New York City            New York  United States       US     East   \n",
      "1          Wollongong     New South Wales      Australia     APAC  Oceania   \n",
      "2            Brisbane          Queensland      Australia     APAC  Oceania   \n",
      "3              Berlin              Berlin        Germany       EU  Central   \n",
      "4               Dakar               Dakar        Senegal   Africa   Africa   \n",
      "...               ...                 ...            ...      ...      ...   \n",
      "51032         Abilene               Texas  United States       US  Central   \n",
      "51043        Felahiye             Kayseri         Turkey     EMEA     EMEA   \n",
      "51084        Lewiston               Idaho  United States       US     West   \n",
      "51176  Victoria Falls  Matabeleland North       Zimbabwe   Africa   Africa   \n",
      "51290         Unknown             Unknown        Unknown  Unknown  Unknown   \n",
      "\n",
      "      postal_code  geo_key  \n",
      "0           10024        1  \n",
      "1                        2  \n",
      "2                        3  \n",
      "3                        4  \n",
      "4                        5  \n",
      "...           ...      ...  \n",
      "51032       79605     3844  \n",
      "51043                 3845  \n",
      "51084       83501     3846  \n",
      "51176                 3847  \n",
      "51290     Unknown     3848  \n",
      "\n",
      "[3848 rows x 7 columns]\n",
      "               order_id  order_date_id  ship_date_id  customer_key  \\\n",
      "0        CA-2012-124891       20120731      20120731             1   \n",
      "1        CA-2012-124891       20120731      20120731             1   \n",
      "2        CA-2012-124891       20120731      20120731             1   \n",
      "3        CA-2012-124891       20120731      20120731             1   \n",
      "4        CA-2012-124891       20120731      20120731          1925   \n",
      "...                 ...            ...           ...           ...   \n",
      "8896123  MX-2012-134460       20120522      20120526         19718   \n",
      "8896124  MX-2012-134460       20120522      20120526         22396   \n",
      "8896125  MX-2012-134460       20120522      20120526         24413   \n",
      "8896126  MX-2012-134460       20120522      20120526         24959   \n",
      "8896127         UNKNOWN       19000101      19000101            -1   \n",
      "\n",
      "              product_id  geo_key  quantity  sales_amount  discount    profit  \\\n",
      "0        TEC-AC-10003033        1       7.0       2309.65       0.0  762.1845   \n",
      "1        TEC-AC-10003033       37       7.0       2309.65       0.0  762.1845   \n",
      "2        TEC-AC-10003033      131       7.0       2309.65       0.0  762.1845   \n",
      "3        TEC-AC-10003033      407       7.0       2309.65       0.0  762.1845   \n",
      "4        TEC-AC-10003033        1       7.0       2309.65       0.0  762.1845   \n",
      "...                  ...      ...       ...           ...       ...       ...   \n",
      "8896123  OFF-PA-10004155     1421       3.0         61.38       0.0    1.8000   \n",
      "8896124  OFF-PA-10004155     1421       3.0         61.38       0.0    1.8000   \n",
      "8896125  OFF-PA-10004155     1421       3.0         61.38       0.0    1.8000   \n",
      "8896126  OFF-PA-10004155     1421       3.0         61.38       0.0    1.8000   \n",
      "8896127          UNKNOWN       -1       0.0          0.00       0.0    0.0000   \n",
      "\n",
      "         shipping_cost order_priority     ship_mode  total_amount  \\\n",
      "0               933.57       Critical      Same Day      16167.55   \n",
      "1               933.57       Critical      Same Day      16167.55   \n",
      "2               933.57       Critical      Same Day      16167.55   \n",
      "3               933.57       Critical      Same Day      16167.55   \n",
      "4               933.57       Critical      Same Day      16167.55   \n",
      "...                ...            ...           ...           ...   \n",
      "8896123           0.00           High  Second Class        184.14   \n",
      "8896124           0.00           High  Second Class        184.14   \n",
      "8896125           0.00           High  Second Class        184.14   \n",
      "8896126           0.00           High  Second Class        184.14   \n",
      "8896127           0.00        UNKNOWN       UNKNOWN          0.00   \n",
      "\n",
      "         discount_amount  net_amount  \n",
      "0                    0.0    16167.55  \n",
      "1                    0.0    16167.55  \n",
      "2                    0.0    16167.55  \n",
      "3                    0.0    16167.55  \n",
      "4                    0.0    16167.55  \n",
      "...                  ...         ...  \n",
      "8896123              0.0      184.14  \n",
      "8896124              0.0      184.14  \n",
      "8896125              0.0      184.14  \n",
      "8896126              0.0      184.14  \n",
      "8896127              0.0        0.00  \n",
      "\n",
      "[8896128 rows x 16 columns]\n",
      "       product_key   price  rating  rating_count               last_updated\n",
      "0              NaN  109.95     3.9         120.0 2024-12-06 02:48:17.266391\n",
      "1              NaN   22.30     4.1         259.0 2024-12-06 02:48:17.266391\n",
      "2              NaN   55.99     4.7         500.0 2024-12-06 02:48:17.266391\n",
      "3              NaN   15.99     2.1         430.0 2024-12-06 02:48:17.266391\n",
      "4              NaN  695.00     4.6         400.0 2024-12-06 02:48:17.266391\n",
      "5              NaN  168.00     3.9          70.0 2024-12-06 02:48:17.266391\n",
      "6              NaN    9.99     3.0         400.0 2024-12-06 02:48:17.266391\n",
      "7              NaN   10.99     1.9         100.0 2024-12-06 02:48:17.266391\n",
      "8              NaN   64.00     3.3         203.0 2024-12-06 02:48:17.266391\n",
      "9              NaN  109.00     2.9         470.0 2024-12-06 02:48:17.266391\n",
      "10             NaN  109.00     4.8         319.0 2024-12-06 02:48:17.266391\n",
      "11             NaN  114.00     4.8         400.0 2024-12-06 02:48:17.266391\n",
      "12             NaN  599.00     2.9         250.0 2024-12-06 02:48:17.266391\n",
      "13             NaN  999.99     2.2         140.0 2024-12-06 02:48:17.266391\n",
      "14             NaN   56.99     2.6         235.0 2024-12-06 02:48:17.266391\n",
      "15             NaN   29.95     2.9         340.0 2024-12-06 02:48:17.266391\n",
      "16             NaN   39.99     3.8         679.0 2024-12-06 02:48:17.266391\n",
      "17             NaN    9.85     4.7         130.0 2024-12-06 02:48:17.266391\n",
      "18             NaN    7.95     4.5         146.0 2024-12-06 02:48:17.266391\n",
      "19             NaN   12.99     3.6         145.0 2024-12-06 02:48:17.266391\n",
      "10769      10770.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10770      10771.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10771      10772.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10772      10773.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10773      10774.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10774      10775.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10775      10776.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10776      10777.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10777      10778.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10778      10779.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10779      10780.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10780      10781.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10781      10782.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10782      10783.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10783      10784.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10784      10785.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10785      10786.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10786      10787.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10787      10788.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n",
      "10788      10789.0     NaN     NaN           NaN 2024-12-06 02:48:17.266391\n"
     ]
    }
   ],
   "source": [
    "def create_unified_product_dimension(sales_df: pd.DataFrame, api_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create unified product dimension from both sources\"\"\"\n",
    "    # Transform sales products\n",
    "    sales_products = sales_df[[\n",
    "        'Product ID', 'Product Name', 'Category', 'Sub-Category'\n",
    "    ]].drop_duplicates()\n",
    "    sales_products['source'] = 'sales'\n",
    "    sales_products.columns = ['product_id', 'product_name', 'category', 'subcategory', 'source']\n",
    "\n",
    "    # Transform API products\n",
    "    api_products = pd.DataFrame({\n",
    "        'product_id': 'API_' + api_df['id'].astype(str),\n",
    "        'product_name': api_df['title'],\n",
    "        'category': api_df['category'],\n",
    "        'subcategory': 'N/A',  # FakeStore API doesn't have subcategories\n",
    "        'source': 'api'\n",
    "    })\n",
    "\n",
    "    # Combine products from both sources\n",
    "    unified_products = pd.concat([sales_products, api_products], ignore_index=True)\n",
    "    unified_products['product_key'] = range(1, len(unified_products) + 1)\n",
    "    \n",
    "    return unified_products\n",
    "\n",
    "def create_fact_product_inventory(api_df: pd.DataFrame, dim_product: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create product inventory fact table from API data\"\"\"\n",
    "    # Extract rating data into separate columns\n",
    "    api_df['rating_rate'] = api_df['rating'].apply(lambda x: x['rate'])\n",
    "    api_df['rating_count'] = api_df['rating'].apply(lambda x: x['count'])\n",
    "\n",
    "    # Create inventory fact records\n",
    "    inventory_facts = pd.DataFrame({\n",
    "        'product_key': dim_product[dim_product['source'] == 'api']['product_key'],\n",
    "        'price': api_df['price'],\n",
    "        'rating': api_df['rating_rate'],\n",
    "        'rating_count': api_df['rating_count'],\n",
    "        'last_updated': pd.Timestamp.now()\n",
    "    })\n",
    "\n",
    "    return inventory_facts\n",
    "\n",
    "def create_date_dimension( df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create date dimension with proper null handling\"\"\"\n",
    "    # Convert dates to datetime, handling NaN values\n",
    "    order_dates = pd.to_datetime(df['Order Date'], format='%d-%m-%Y', errors='coerce')\n",
    "    ship_dates = pd.to_datetime(df['Ship Date'], format='%d-%m-%Y', errors='coerce')\n",
    "    \n",
    "    # Combine and get unique dates, dropping NaN\n",
    "    all_dates = pd.concat([order_dates, ship_dates]).dropna().unique()\n",
    "    \n",
    "    if len(all_dates) == 0:\n",
    "        # If no valid dates, create a default date\n",
    "        all_dates = [pd.Timestamp('1900-01-01')]\n",
    "        \n",
    "    all_dates = pd.DatetimeIndex(sorted(all_dates))\n",
    "    \n",
    "    # Create date dimension\n",
    "    dim_date = pd.DataFrame({\n",
    "        'date_id': all_dates.strftime('%Y%m%d').astype(int),\n",
    "        'date': all_dates,\n",
    "        'year': all_dates.year,\n",
    "        'quarter': all_dates.quarter,\n",
    "        'month': all_dates.month,\n",
    "        'month_name': all_dates.strftime('%B'),\n",
    "        'week': all_dates.isocalendar().week,\n",
    "        'day': all_dates.day,\n",
    "        'weekday': all_dates.dayofweek,\n",
    "        'weekday_name': all_dates.strftime('%A'),\n",
    "        'is_weekend': all_dates.dayofweek.isin([5, 6]).astype(int)\n",
    "    })\n",
    "    \n",
    "    return dim_date\n",
    "    \n",
    "def create_customer_dimension(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create customer dimension with proper null handling\"\"\"\n",
    "    customer_cols = [\n",
    "        'Customer ID', 'Customer Name', 'Segment',\n",
    "        'City', 'State', 'Country', 'Market', 'Region'\n",
    "    ]\n",
    "    \n",
    "    dim_customer = df[customer_cols].drop_duplicates()\n",
    "    dim_customer = dim_customer.fillna({\n",
    "        'Customer ID': 'UNKNOWN',\n",
    "        'Customer Name': 'Unknown',\n",
    "        'Segment': 'Unknown',\n",
    "        'City': 'Unknown',\n",
    "        'State': 'Unknown',\n",
    "        'Country': 'Unknown',\n",
    "        'Market': 'Unknown',\n",
    "        'Region': 'Unknown'\n",
    "    })\n",
    "    \n",
    "    # Rename columns\n",
    "    dim_customer.columns = [\n",
    "        'customer_id', 'customer_name', 'segment',\n",
    "        'city', 'state', 'country', 'market', 'region'\n",
    "    ]\n",
    "    \n",
    "    # Add customer key\n",
    "    dim_customer['customer_key'] = range(1, len(dim_customer) + 1)\n",
    "    \n",
    "    return dim_customer\n",
    "    \n",
    "def create_geography_dimension(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create geography dimension with proper null handling\"\"\"\n",
    "    geo_cols = ['City', 'State', 'Country', 'Market', 'Region', 'Postal Code']\n",
    "    \n",
    "    dim_geography = df[geo_cols].drop_duplicates()\n",
    "    dim_geography = dim_geography.fillna({\n",
    "        'City': 'Unknown',\n",
    "        'State': 'Unknown',\n",
    "        'Country': 'Unknown',\n",
    "        'Market': 'Unknown',\n",
    "        'Region': 'Unknown',\n",
    "        'Postal Code': 'Unknown'\n",
    "    })\n",
    "    \n",
    "    # Rename columns\n",
    "    dim_geography.columns = [\n",
    "        'city', 'state', 'country', 'market', 'region', 'postal_code'\n",
    "    ]\n",
    "    \n",
    "    # Add geography key\n",
    "    dim_geography['geo_key'] = range(1, len(dim_geography) + 1)\n",
    "    \n",
    "    return dim_geography\n",
    "    \n",
    "def create_fact_sales(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create sales fact table from sales data using merge operations\"\"\"\n",
    "    # Create a working copy\n",
    "    working_df = df.copy()\n",
    "    \n",
    "    # Convert dates to date_id format\n",
    "    working_df['order_date_id'] = pd.to_datetime(\n",
    "        working_df['Order Date'], \n",
    "        format='%d-%m-%Y', \n",
    "        errors='coerce'\n",
    "    ).fillna(pd.Timestamp('1900-01-01')).dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    working_df['ship_date_id'] = pd.to_datetime(\n",
    "        working_df['Ship Date'], \n",
    "        format='%d-%m-%Y', \n",
    "        errors='coerce'\n",
    "    ).fillna(pd.Timestamp('1900-01-01')).dt.strftime('%Y%m%d').astype(int)\n",
    "    \n",
    "    # Get dimension keys through merges\n",
    "    dim_customer = create_customer_dimension(working_df)\n",
    "    working_df = working_df.merge(\n",
    "        dim_customer[['customer_id', 'customer_key']],\n",
    "        left_on='Customer ID',\n",
    "        right_on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    dim_geography = create_geography_dimension(working_df)\n",
    "    working_df = working_df.merge(\n",
    "        dim_geography[['city', 'state', 'country', 'geo_key']],\n",
    "        left_on=['City', 'State', 'Country'],\n",
    "        right_on=['city', 'state', 'country'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Create fact table\n",
    "    fact_sales = pd.DataFrame({\n",
    "        'order_id': working_df['Order ID'].fillna('UNKNOWN'),\n",
    "        'order_date_id': working_df['order_date_id'],\n",
    "        'ship_date_id': working_df['ship_date_id'],\n",
    "        'customer_key': working_df['customer_key'].fillna(-1).astype(int),\n",
    "        'product_id': working_df['Product ID'].fillna('UNKNOWN'),  # Changed from product_key to product_id\n",
    "        'geo_key': working_df['geo_key'].fillna(-1).astype(int),\n",
    "        'quantity': pd.to_numeric(working_df['Quantity'], errors='coerce').fillna(0),\n",
    "        'sales_amount': pd.to_numeric(working_df['Sales'], errors='coerce').fillna(0),\n",
    "        'discount': pd.to_numeric(working_df['Discount'], errors='coerce').fillna(0),\n",
    "        'profit': pd.to_numeric(working_df['Profit'], errors='coerce').fillna(0),\n",
    "        'shipping_cost': pd.to_numeric(working_df['Shipping Cost'], errors='coerce').fillna(0),\n",
    "        'order_priority': working_df['Order Priority'].fillna('UNKNOWN'),\n",
    "        'ship_mode': working_df['Ship Mode'].fillna('UNKNOWN')\n",
    "    })\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    fact_sales['total_amount'] = fact_sales['quantity'] * fact_sales['sales_amount']\n",
    "    fact_sales['discount_amount'] = fact_sales['total_amount'] * fact_sales['discount']\n",
    "    fact_sales['net_amount'] = fact_sales['total_amount'] - fact_sales['discount_amount']\n",
    "    \n",
    "    return fact_sales\n",
    "\n",
    "def transform(data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Transform data into dimensional model\"\"\"\n",
    "    logger.info(\"Transforming data...\")\n",
    "\n",
    "    # Create unified product dimension combining both sources\n",
    "    dim_product = create_unified_product_dimension(\n",
    "        data['sales_data'], \n",
    "        data['api_products']\n",
    "    )\n",
    "\n",
    "    # Create other dimensions\n",
    "    dim_date = create_date_dimension(data['sales_data'])\n",
    "    dim_customer = create_customer_dimension(data['sales_data'])\n",
    "    dim_geography = create_geography_dimension(data['sales_data'])\n",
    "\n",
    "    # Create fact tables\n",
    "    fact_sales = create_fact_sales(data['sales_data'])\n",
    "    fact_product_inventory = create_fact_product_inventory(\n",
    "        data['api_products'], \n",
    "        dim_product\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'dim_date': dim_date,\n",
    "        'dim_customer': dim_customer,\n",
    "        'dim_product': dim_product,\n",
    "        'dim_geography': dim_geography,\n",
    "        'fact_sales': fact_sales,\n",
    "        'fact_product_inventory': fact_product_inventory\n",
    "    }\n",
    "    \n",
    "try:\n",
    "    # 1. MongoDB extraction (sales data)\n",
    "    df_mongo = extract_from_mongodb(\"sales_orders\")\n",
    "    print(\"MongoDB Data:\")\n",
    "    print(df_mongo)\n",
    "    \n",
    "    # 2. FakeStore API extraction\n",
    "    api_products = get_fakestore_products()\n",
    "    api_products_df = pd.DataFrame(api_products)\n",
    "    print(\"Api Data:\", api_products_df)\n",
    "    print(df_mongo)\n",
    "    logger.info(f\"Extracted {len(api_products_df)} products from FakeStore API\")\n",
    "    \n",
    "    # 3. Transform data into dimensional model\n",
    "    raw_data = {\n",
    "        'sales_data': df_mongo,\n",
    "        'api_products': api_products_df\n",
    "    }\n",
    "    transformed_tables = transform(raw_data)  \n",
    "    [print(table) for table in transformed_tables.values()]\n",
    "    \n",
    "    # # 4. Load transformed data into MySQL database\n",
    "    # load_to_mysql(transformed_tables)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in MongoDB extraction test: {str(e)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "This class provides the implementation of load_to_warehouse, simulating loading a DataFrame into a data warehouse. It connects to the MSQL database and load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "class MsqlDatabaseConnection:\n",
    "    def __init__(self, username: str, password: str, host: str = \"localhost\", port: int = 3306, db_name: str = \"myshop\"):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.db_name = db_name\n",
    "\n",
    "    def get_sqlalchemy_engine(self) -> Engine:\n",
    "        \"\"\"Return a SQLAlchemy engine connected to MySQL database with UTF-8 support.\"\"\"\n",
    "        try:\n",
    "            connection_uri = f\"mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.db_name}?charset=utf8mb4\"\n",
    "            engine = create_engine(\n",
    "                connection_uri,\n",
    "                connect_args={'charset': 'utf8mb4'}\n",
    "            )\n",
    "            logger.info(\"Successfully connected to MySQL database.\")\n",
    "            return engine\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error connecting to MySQL: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_query_result(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute a query and return the result as a DataFrame.\"\"\"\n",
    "        try:\n",
    "            engine = self.get_sqlalchemy_engine()\n",
    "            result = pd.read_sql(query, engine)\n",
    "            logger.info(f\"Query executed successfully: {query}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing query: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# DataLoader class\n",
    "class DataLoader:\n",
    "    def __init__(self, db_connection: MsqlDatabaseConnection):\n",
    "        self.db_conn = db_connection\n",
    "    \n",
    "    def load_to_warehouse(self, df: pd.DataFrame, table_name: str, if_exists: str = 'append') -> None:\n",
    "        \"\"\"Load DataFrame to data warehouse.\"\"\"\n",
    "        try:\n",
    "            engine = self.db_conn.get_sqlalchemy_engine()\n",
    "            df.to_sql(\n",
    "                name=table_name,\n",
    "                con=engine,\n",
    "                if_exists=if_exists,\n",
    "                index=False,\n",
    "                chunksize=1000\n",
    "            )\n",
    "            logger.info(f\"Successfully loaded {len(df)} rows to {table_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data to warehouse: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "db_connection = MsqlDatabaseConnection(\n",
    "    username=\"root\",  # Replace with your MySQL username\n",
    "    password=\"Akinolami6650!\",  # Replace with your MySQL password\n",
    "    db_name=\"myshop\"           # The name of your database\n",
    ")\n",
    "\n",
    "# Create an instance of DataLoader\n",
    "data_loader = DataLoader(db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB client initialized successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongo_db Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'data_ware_house')\n",
      "MongoDB Data extracted successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Transforming data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Data extracted successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading dim_date...\n",
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Successfully loaded 1468 rows to dim_date\n",
      "INFO:__main__:Loading dim_customer...\n",
      "INFO:__main__:Successfully connected to MySQL database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded 24960 rows to dim_customer\n",
      "INFO:__main__:Loading dim_product...\n",
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Successfully loaded 10789 rows to dim_product\n",
      "INFO:__main__:Loading dim_geography...\n",
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Successfully loaded 3848 rows to dim_geography\n",
      "INFO:__main__:Loading fact_sales...\n",
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Successfully loaded 8896128 rows to fact_sales\n",
      "INFO:__main__:Loading fact_product_inventory...\n",
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Successfully loaded 40 rows to fact_product_inventory\n",
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: SELECT COUNT(*) as customer_count FROM dim_customer\n",
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: SELECT COUNT(*) as product_count FROM dim_product\n",
      "INFO:__main__:Successfully connected to MySQL database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed\n",
      "\n",
      "Verification Results:\n",
      "\n",
      "Customer Count:\n",
      "   customer_count\n",
      "0           24960\n",
      "\n",
      "Product Count:\n",
      "   product_count\n",
      "0          10789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Query executed successfully: SELECT COUNT(*) as sales_count, SUM(sales_amount) as total_sales FROM fact_sales\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sales:\n",
      "   sales_count   total_sales\n",
      "0      8896128  2.262738e+09\n"
     ]
    }
   ],
   "source": [
    "def load_to_mysql(tables: Dict[str, pd.DataFrame], data_loader: DataLoader):\n",
    "    \"\"\"Load all tables into MySQL warehouse\"\"\"\n",
    "    try:\n",
    "        # Define loading order (dimensions first, then facts)\n",
    "        load_order = [\n",
    "            'dim_date',\n",
    "            'dim_customer',\n",
    "            'dim_product',\n",
    "            'dim_geography',\n",
    "            'fact_sales',\n",
    "            'fact_product_inventory'\n",
    "        ]\n",
    "\n",
    "        for table_name in load_order:\n",
    "            if table_name in tables:\n",
    "                logger.info(f\"Loading {table_name}...\")\n",
    "                data_loader.load_to_warehouse(\n",
    "                    df=tables[table_name],\n",
    "                    table_name=table_name,\n",
    "                    if_exists='replace'  # Using replace to refresh the tables\n",
    "                )\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in load_to_mysql: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # 1. MongoDB extraction (sales data)\n",
    "    df_mongo = extract_from_mongodb(\"sales_orders\")\n",
    "    print(\"MongoDB Data extracted successfully\")\n",
    "    \n",
    "    # 2. FakeStore API extraction\n",
    "    api_products = get_fakestore_products()\n",
    "    api_products_df = pd.DataFrame(api_products)\n",
    "    print(\"API Data extracted successfully\")\n",
    "    \n",
    "    # 3. Transform data into dimensional model\n",
    "    raw_data = {\n",
    "        'sales_data': df_mongo,\n",
    "        'api_products': api_products_df\n",
    "    }\n",
    "    transformed_tables = transform(raw_data)\n",
    "    print(\"Data transformation completed\")\n",
    "    \n",
    "    # 4. Load transformed data into MySQL database\n",
    "    db_connection = MsqlDatabaseConnection(\n",
    "        username=\"root\",\n",
    "        password=\"Akinolami6650!\",\n",
    "        db_name=\"myshop\"\n",
    "    )\n",
    "    data_loader = DataLoader(db_connection)\n",
    "    \n",
    "    # Load the data\n",
    "    load_to_mysql(transformed_tables, data_loader)\n",
    "    print(\"Data loading completed\")\n",
    "    \n",
    "    # 5. Verify the load with some basic queries\n",
    "    verification_queries = {\n",
    "        \"Customer Count\": \"SELECT COUNT(*) as customer_count FROM dim_customer\",\n",
    "        \"Product Count\": \"SELECT COUNT(*) as product_count FROM dim_product\",\n",
    "        \"Total Sales\": \"SELECT COUNT(*) as sales_count, SUM(sales_amount) as total_sales FROM fact_sales\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nVerification Results:\")\n",
    "    for name, query in verification_queries.items():\n",
    "        result = db_connection.get_query_result(query)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in ETL pipeline: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Analysis by Customer Segment and Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: \n",
      "SELECT \n",
      "    d.year,\n",
      "    d.quarter,\n",
      "    COUNT(DISTINCT fs.order_id) as total_orders,\n",
      "    COUNT(DISTINCT fs.customer_key) as unique_customers,\n",
      "    SUM(fs.sales_amount) as total_sales,\n",
      "    AVG(fs.sales_amount) as avg_sale_per_order,\n",
      "    STDDEV(fs.sales_amount) as sales_std_dev,\n",
      "    MIN(fs.sales_amount) as min_sale,\n",
      "    MAX(fs.sales_amount) as max_sale,\n",
      "    SUM(fs.profit) as total_profit,\n",
      "    AVG(fs.profit) as avg_profit_per_order,\n",
      "    STDDEV(fs.profit) as profit_std_dev,\n",
      "    AVG(fs.discount) * 100 as avg_discount_percentage,\n",
      "    SUM(fs.shipping_cost) as total_shipping_cost\n",
      "FROM fact_sales fs\n",
      "JOIN dim_date d ON fs.order_date_id = d.date_id\n",
      "GROUP BY d.year, d.quarter\n",
      "ORDER BY d.year, d.quarter;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Analysis by Customer Segment and Quarter:\n",
      "    year  quarter  total_orders  unique_customers   total_sales  \\\n",
      "0   2011        1           673             10745  5.264300e+07   \n",
      "1   2011        2          1027             14062  8.451945e+07   \n",
      "2   2011        3          1217             15664  1.164795e+08   \n",
      "3   2011        4          1581             18100  1.580572e+08   \n",
      "4   2012        1           826             12663  7.226945e+07   \n",
      "5   2012        2          1297             16453  1.067034e+08   \n",
      "6   2012        3          1480             17552  1.338895e+08   \n",
      "7   2012        4          1837             18945  1.722617e+08   \n",
      "8   2013        1          1059             14485  9.578419e+07   \n",
      "9   2013        2          1635             18361  1.524045e+08   \n",
      "10  2013        3          1962             19848  1.647233e+08   \n",
      "11  2013        4          2184             19978  1.994881e+08   \n",
      "12  2014        1          1355             16845  1.195761e+08   \n",
      "13  2014        2          2077             19959  1.565714e+08   \n",
      "14  2014        3          2379             21009  2.134718e+08   \n",
      "15  2014        4          2959             22042  2.638953e+08   \n",
      "\n",
      "    avg_sale_per_order  sales_std_dev  min_sale   max_sale  total_profit  \\\n",
      "0           248.241104     710.093995     0.852  22638.480  4.835288e+06   \n",
      "1           240.172103     423.415558     1.365   4624.290  9.537556e+06   \n",
      "2           258.446973     575.389913     0.876   9449.950  1.249184e+07   \n",
      "3           265.507581     509.330936     0.898   6999.960  1.995777e+07   \n",
      "4           270.041008     515.164875     1.112   6354.950  8.563622e+06   \n",
      "5           240.817525     449.066412     0.984   5724.540  1.508037e+07   \n",
      "6           254.338645     474.256911     1.080   5759.964  1.505866e+07   \n",
      "7           256.307485     473.036025     1.566   5785.020  2.173744e+07   \n",
      "8           279.462776     589.357169     1.272   8749.950  1.135843e+07   \n",
      "9           255.340317     562.410394     0.836   9099.930  1.601195e+07   \n",
      "10          239.556350     439.769609     1.668   5211.120  1.863582e+07   \n",
      "11          259.316573     581.226260     1.161  17499.950  2.962775e+07   \n",
      "12          263.431071     622.656723     0.556  13999.960  1.685362e+07   \n",
      "13          230.833143     440.420171     0.444   5486.670  1.904886e+07   \n",
      "14          264.232418     501.390480     1.080   7958.580  2.579442e+07   \n",
      "15          253.668451     540.779476     0.990  11199.968  3.004010e+07   \n",
      "\n",
      "    avg_profit_per_order  profit_std_dev  avg_discount_percentage  \\\n",
      "0              22.801078      178.062207                14.468832   \n",
      "1              27.102104      130.626513                14.799209   \n",
      "2              27.717138      180.716945                13.885741   \n",
      "3              33.525452      177.688148                14.558791   \n",
      "4              31.998709      175.382777                13.535347   \n",
      "5              34.034705      151.781233                13.972598   \n",
      "6              28.605685      149.908037                14.511164   \n",
      "7              32.343049      166.793281                13.440682   \n",
      "8              33.139680      181.312883                13.332732   \n",
      "9              26.826619      174.850497                14.231342   \n",
      "10             27.102002      143.548235                14.098059   \n",
      "11             38.513410      241.827785                13.547438   \n",
      "12             37.129221      254.778297                13.471648   \n",
      "13             28.083728      152.361507                14.715148   \n",
      "14             31.927975      157.924143                13.933062   \n",
      "15             28.875938      205.502284                14.481564   \n",
      "\n",
      "    total_shipping_cost  \n",
      "0          5.226735e+06  \n",
      "1          9.250970e+06  \n",
      "2          1.198071e+07  \n",
      "3          1.792932e+07  \n",
      "4          7.326519e+06  \n",
      "5          1.176609e+07  \n",
      "6          1.441672e+07  \n",
      "7          1.806648e+07  \n",
      "8          9.929282e+06  \n",
      "9          1.616481e+07  \n",
      "10         1.696947e+07  \n",
      "11         2.098387e+07  \n",
      "12         1.184128e+07  \n",
      "13         1.742226e+07  \n",
      "14         2.316680e+07  \n",
      "15         2.816892e+07  \n"
     ]
    }
   ],
   "source": [
    "#1. Sales Performance Analysis by Time Period\n",
    "sales_by_segment_query = \"\"\"\n",
    "SELECT \n",
    "    d.year,\n",
    "    d.quarter,\n",
    "    COUNT(DISTINCT fs.order_id) as total_orders,\n",
    "    COUNT(DISTINCT fs.customer_key) as unique_customers,\n",
    "    SUM(fs.sales_amount) as total_sales,\n",
    "    AVG(fs.sales_amount) as avg_sale_per_order,\n",
    "    STDDEV(fs.sales_amount) as sales_std_dev,\n",
    "    MIN(fs.sales_amount) as min_sale,\n",
    "    MAX(fs.sales_amount) as max_sale,\n",
    "    SUM(fs.profit) as total_profit,\n",
    "    AVG(fs.profit) as avg_profit_per_order,\n",
    "    STDDEV(fs.profit) as profit_std_dev,\n",
    "    AVG(fs.discount) * 100 as avg_discount_percentage,\n",
    "    SUM(fs.shipping_cost) as total_shipping_cost\n",
    "FROM fact_sales fs\n",
    "JOIN dim_date d ON fs.order_date_id = d.date_id\n",
    "GROUP BY d.year, d.quarter\n",
    "ORDER BY d.year, d.quarter;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_segment_analysis = db_connection.get_query_result(sales_by_segment_query)\n",
    "    print(\"Sales Analysis by Customer Segment and Quarter:\")\n",
    "    print(df_segment_analysis)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in segment analysis query: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Customer Segment Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "ERROR:__main__:Error executing query: (pymysql.err.InternalError) (3, \"Error writing file '/var/tmp/MYbUq5Pn' (OS errno 28 - No space left on device)\")\n",
      "[SQL: \n",
      "SELECT \n",
      "    dp.category,\n",
      "    dp.subcategory,\n",
      "    COUNT(DISTINCT fs.order_id) as total_orders,\n",
      "    SUM(fs.quantity) as total_units_sold,\n",
      "    AVG(fs.quantity) as avg_units_per_order,\n",
      "    STDDEV(fs.quantity) as quantity_std_dev,\n",
      "    SUM(fs.sales_amount) as total_revenue,\n",
      "    AVG(fs.sales_amount) as avg_revenue_per_order,\n",
      "    STDDEV(fs.sales_amount) as revenue_std_dev,\n",
      "    AVG(fpi.rating) as avg_product_rating,\n",
      "    STDDEV(fpi.rating) as rating_std_dev,\n",
      "    AVG(fpi.price) as avg_list_price,\n",
      "    STDDEV(fpi.price) as price_std_dev\n",
      "FROM dim_product dp\n",
      "LEFT JOIN fact_sales fs ON dp.product_id = fs.product_id  -- Changed from product_key to product_id\n",
      "LEFT JOIN fact_product_inventory fpi ON dp.product_key = fpi.product_key\n",
      "GROUP BY dp.category, dp.subcategory\n",
      "ORDER BY total_revenue DESC;\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "ERROR:__main__:Error in product performance analysis query: (pymysql.err.InternalError) (3, \"Error writing file '/var/tmp/MYbUq5Pn' (OS errno 28 - No space left on device)\")\n",
      "[SQL: \n",
      "SELECT \n",
      "    dp.category,\n",
      "    dp.subcategory,\n",
      "    COUNT(DISTINCT fs.order_id) as total_orders,\n",
      "    SUM(fs.quantity) as total_units_sold,\n",
      "    AVG(fs.quantity) as avg_units_per_order,\n",
      "    STDDEV(fs.quantity) as quantity_std_dev,\n",
      "    SUM(fs.sales_amount) as total_revenue,\n",
      "    AVG(fs.sales_amount) as avg_revenue_per_order,\n",
      "    STDDEV(fs.sales_amount) as revenue_std_dev,\n",
      "    AVG(fpi.rating) as avg_product_rating,\n",
      "    STDDEV(fpi.rating) as rating_std_dev,\n",
      "    AVG(fpi.price) as avg_list_price,\n",
      "    STDDEV(fpi.price) as price_std_dev\n",
      "FROM dim_product dp\n",
      "LEFT JOIN fact_sales fs ON dp.product_id = fs.product_id  -- Changed from product_key to product_id\n",
      "LEFT JOIN fact_product_inventory fpi ON dp.product_key = fpi.product_key\n",
      "GROUP BY dp.category, dp.subcategory\n",
      "ORDER BY total_revenue DESC;\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "product_performance_query = \"\"\"\n",
    "SELECT \n",
    "    dp.category,\n",
    "    dp.subcategory,\n",
    "    COUNT(DISTINCT fs.order_id) as total_orders,\n",
    "    SUM(fs.quantity) as total_units_sold,\n",
    "    AVG(fs.quantity) as avg_units_per_order,\n",
    "    STDDEV(fs.quantity) as quantity_std_dev,\n",
    "    SUM(fs.sales_amount) as total_revenue,\n",
    "    AVG(fs.sales_amount) as avg_revenue_per_order,\n",
    "    STDDEV(fs.sales_amount) as revenue_std_dev,\n",
    "    AVG(fpi.rating) as avg_product_rating,\n",
    "    STDDEV(fpi.rating) as rating_std_dev,\n",
    "    AVG(fpi.price) as avg_list_price,\n",
    "    STDDEV(fpi.price) as price_std_dev\n",
    "FROM dim_product dp\n",
    "LEFT JOIN fact_sales fs ON dp.product_id = fs.product_id  -- Changed from product_key to product_id\n",
    "LEFT JOIN fact_product_inventory fpi ON dp.product_key = fpi.product_key\n",
    "GROUP BY dp.category, dp.subcategory\n",
    "ORDER BY total_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_segment_analysis = db_connection.get_query_result(product_performance_query)\n",
    "    print(\"Product Performance and Inventory Analysis:\")\n",
    "    print(df_segment_analysis)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in product performance analysis query: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Sales Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: \n",
      "SELECT \n",
      "    c.country_region as Country,\n",
      "    c.city as City,\n",
      "    COUNT(DISTINCT o.customer_id) as Unique_Customers,\n",
      "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Sales,\n",
      "    AVG(od.quantity * od.unit_price * (1 - od.discount)) as Avg_Order_Value,\n",
      "    SUM(od.quantity) as Total_Units_Sold\n",
      "FROM orders o\n",
      "JOIN customers c ON o.customer_id = c.id\n",
      "JOIN order_details od ON o.id = od.order_id\n",
      "GROUP BY c.country_region, c.city\n",
      "HAVING Total_Sales > 1000\n",
      "ORDER BY Total_Sales DESC;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Geographic Sales Distribution:\n",
      "   Country            City  Unique_Customers  Total_Sales  Avg_Order_Value  \\\n",
      "0      USA         Memphis                 1     15432.50      3858.125000   \n",
      "1      USA           Boise                 1     13800.00     13800.000000   \n",
      "2      USA       Milwaukee                 1      8007.50      1334.583333   \n",
      "3      USA        New York                 1      4949.00       707.000000   \n",
      "4      USA        Portland                 1      4683.00       780.500000   \n",
      "5      USA           Miami                 2      4644.75       663.535714   \n",
      "6      USA  Salt Lake City                 1      3786.50      1262.166667   \n",
      "7      USA          Denver                 1      2905.50       968.500000   \n",
      "8      USA       Las Vegas                 2      2695.00       673.750000   \n",
      "9      USA     Los Angelas                 1      2550.00       510.000000   \n",
      "10     USA         Seattle                 1      2410.75       602.687500   \n",
      "11     USA         Chicago                 2      2272.50       284.062500   \n",
      "\n",
      "    Total_Units_Sold  \n",
      "0              405.0  \n",
      "1              300.0  \n",
      "2              427.0  \n",
      "3              140.0  \n",
      "4              260.0  \n",
      "5              265.0  \n",
      "6              160.0  \n",
      "7              137.0  \n",
      "8              165.0  \n",
      "9              253.0  \n",
      "10             115.0  \n",
      "11             315.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "geographic_sales_query = \"\"\"\n",
    "SELECT \n",
    "    c.country_region as Country,\n",
    "    c.city as City,\n",
    "    COUNT(DISTINCT o.customer_id) as Unique_Customers,\n",
    "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Sales,\n",
    "    AVG(od.quantity * od.unit_price * (1 - od.discount)) as Avg_Order_Value,\n",
    "    SUM(od.quantity) as Total_Units_Sold\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.id\n",
    "JOIN order_details od ON o.id = od.order_id\n",
    "GROUP BY c.country_region, c.city\n",
    "HAVING Total_Sales > 1000\n",
    "ORDER BY Total_Sales DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_geographic_sales = db_connection.get_query_result(geographic_sales_query)\n",
    "    print(\"\\nGeographic Sales Distribution:\")\n",
    "    print(df_geographic_sales)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in geographic sales query: {str(e)}\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shipping Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully connected to MySQL database.\n",
      "INFO:__main__:Query executed successfully: \n",
      "SELECT \n",
      "    s.company as Ship_Mode,\n",
      "    COUNT(DISTINCT o.id) as Total_Shipments,\n",
      "    AVG(DATEDIFF(o.shipped_date, o.order_date)) as Avg_Days_to_Ship,\n",
      "    SUM(o.shipping_fee) as Total_Shipping_Cost,\n",
      "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Order_Value,\n",
      "    (SUM(o.shipping_fee) / SUM(od.quantity * od.unit_price * (1 - od.discount))) * 100 as Shipping_Cost_Percentage\n",
      "FROM orders o\n",
      "JOIN shippers s ON o.shipper_id = s.id\n",
      "JOIN order_details od ON o.id = od.order_id\n",
      "WHERE o.shipped_date IS NOT NULL\n",
      "GROUP BY s.company\n",
      "ORDER BY Total_Shipments DESC;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shipping Performance Analysis:\n",
      "            Ship_Mode  Total_Shipments  Avg_Days_to_Ship  Total_Shipping_Cost  \\\n",
      "0  Shipping Company B               14            1.1667               1618.0   \n",
      "1  Shipping Company C               11            0.3333                479.0   \n",
      "2  Shipping Company A                7            3.6923                335.0   \n",
      "\n",
      "   Total_Order_Value  Shipping_Cost_Percentage  \n",
      "0           16078.50                 10.063128  \n",
      "1           24802.25                  1.931276  \n",
      "2            9593.50                  3.491948  \n"
     ]
    }
   ],
   "source": [
    "shipping_performance_query = \"\"\"\n",
    "SELECT \n",
    "    s.company as Ship_Mode,\n",
    "    COUNT(DISTINCT o.id) as Total_Shipments,\n",
    "    AVG(DATEDIFF(o.shipped_date, o.order_date)) as Avg_Days_to_Ship,\n",
    "    SUM(o.shipping_fee) as Total_Shipping_Cost,\n",
    "    SUM(od.quantity * od.unit_price * (1 - od.discount)) as Total_Order_Value,\n",
    "    (SUM(o.shipping_fee) / SUM(od.quantity * od.unit_price * (1 - od.discount))) * 100 as Shipping_Cost_Percentage\n",
    "FROM orders o\n",
    "JOIN shippers s ON o.shipper_id = s.id\n",
    "JOIN order_details od ON o.id = od.order_id\n",
    "WHERE o.shipped_date IS NOT NULL\n",
    "GROUP BY s.company\n",
    "ORDER BY Total_Shipments DESC;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_shipping_performance = db_connection.get_query_result(shipping_performance_query)\n",
    "    print(\"\\nShipping Performance Analysis:\")\n",
    "    print(df_shipping_performance)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in shipping performance query: {str(e)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_dataware_house_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b477158c-c74a-4c2b-951e-de78f169ca39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bff8471-0b62-4bc1-b89a-016b7e8dda5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n  Using cached pymongo-4.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\nCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\nUsing cached pymongo-4.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\nUsing cached dnspython-2.7.0-py3-none-any.whl (313 kB)\nInstalling collected packages: dnspython, pymongo\nSuccessfully installed dnspython-2.7.0 pymongo-4.10.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "%pip install pymongo\n",
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import requests\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "413c8f2c-8baf-4c62-8526-55ffccd41733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe576b8f-41dc-4c71-8ca7-81af5bfc3f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure MySQL Server Connection Information ###################\n",
    "jdbc_hostname = \"upn2vz-mysql.mysql.database.azure.com\"\n",
    "jdbc_port = 3306\n",
    "src_database = \"mystore\"\n",
    "\n",
    "connection_properties = {\n",
    "  \"user\" : \"upn2vzatvirginia\",\n",
    "  \"password\" : \"Akinolami6650!\",\n",
    "  \"driver\" : \"org.mariadb.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "# MongoDB Atlas Connection Information ########################\n",
    "atlas_cluster_name = \"cluster0.mkvm3\"\n",
    "atlas_database_name = \"myshop\"\n",
    "atlas_user_name = \"upn2vz\"\n",
    "atlas_password = \"3yseiHK29Bemx2qK\"\n",
    "\n",
    "\n",
    "# Data Files (JSON) Information ###############################\n",
    "dst_database = \"mystore\"\n",
    "\n",
    "base_dir = \"dbfs:/FileStore/sales_dw\"\n",
    "database_dir = f\"{base_dir}/sales_dw\"\n",
    "# Stream paths\n",
    "stream_dir = f\"{base_dir}/stream\"\n",
    "sales_stream_dir = f\"{stream_dir}/sales\"\n",
    "api_stream_dir = f\"{stream_dir}/products\"\n",
    "# Output paths\n",
    "output_bronze = f\"{database_dir}/bronze\"\n",
    "output_silver = f\"{database_dir}/silver\"\n",
    "output_gold = f\"{database_dir}/gold\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b57db456-3034-44c2-98f5-4daf566b20c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb03afe-8d04-4c55-8342-63382d1f73a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# Use this Function to Fetch a DataFrame from the MongoDB Atlas database server Using PyMongo.\n",
    "##################################################################################################################\n",
    "\n",
    "def get_mongo_dataframe(user_id, pwd, cluster_name, db_name, collection, conditions, projection, sort):\n",
    "    '''Create a client connection to MongoDB'''\n",
    "    mongo_uri = f\"mongodb+srv://{user_id}:{pwd}@{cluster_name}.mongodb.net/{db_name}\"\n",
    "    \n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = client[db_name]\n",
    "    if conditions and projection and sort:\n",
    "        dframe = pd.DataFrame(list(db[collection].find(conditions, projection).sort(sort)))\n",
    "    elif conditions and projection and not sort:\n",
    "        dframe = pd.DataFrame(list(db[collection].find(conditions, projection)))\n",
    "    else:\n",
    "        dframe = pd.DataFrame(list(db[collection].find()))\n",
    "\n",
    "    client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "##################################################################################################################\n",
    "# Use this Function to Create New Collections by Uploading JSON file(s) to the MongoDB Atlas server.\n",
    "##################################################################################################################\n",
    "def set_mongo_collection(user_id, pwd, cluster_name, db_name, src_file_path, json_files):\n",
    "    '''Create a client connection to MongoDB'''\n",
    "    mongo_uri = f\"mongodb+srv://{user_id}:{pwd}@{cluster_name}.mongodb.net/{db_name}\"\n",
    "    client = pymongo.MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    \n",
    "    '''Read in a JSON file, and Use It to Create a New Collection'''\n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(src_file_path, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "\n",
    "    client.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bad96a6-4f66-47a3-b34a-8fd82c8d7edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Section II: Populate Dimensions by Ingesting Reference (Cold-path) Data \n",
    "#### 1.0. Fetch Reference Data From an Azure MySQL Database\n",
    "##### 1.1. Create a New Databricks Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4736a9aa-177a-4f18-9c3c-37a6e91fb7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP DATABASE IF EXISTS mystore CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f37c0a-5082-4fdd-ab7e-846d891c5ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS mystore\n",
    "COMMENT \"DS-2002 Capstone Project Database\"\n",
    "LOCATION \"dbfs:/FileStore/project_data/mystore\"\n",
    "WITH DBPROPERTIES (contains_pii = true, purpose = \"Sales Analysis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb8b4808-587d-485a-b3b3-84736fd73123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1.2. Create a New Table that Sources Date Dimension Data from a Table in an Azure MySQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbbd0a7-fb30-487e-97d4-bc05e60ed5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_date\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"jdbc:mysql://upn2vz-mysql.mysql.database.azure.com:3306/mystore?useSSL=true&requireSSL=true\",\n",
    "  dbtable \"dim_date\",\n",
    "  user \"upn2vzatvirginia\",\n",
    "  password \"Akinolami6650!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3769b6ea-c6fb-4428-8ec9-f7855795e057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 21
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE DATABASE mystore;\n",
    "\n",
    "CREATE OR REPLACE TABLE mystore.dim_date\n",
    "COMMENT \"Date Dimension Table\"\n",
    "LOCATION \"dbfs:/FileStore/lab_data/mystore/dim_date\"\n",
    "AS SELECT * FROM view_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25cf45b-b499-4563-9028-38f60bfd513e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>date_key</td><td>int</td><td>null</td></tr><tr><td>full_date</td><td>date</td><td>null</td></tr><tr><td>date_name</td><td>char(11)</td><td>null</td></tr><tr><td>date_name_us</td><td>char(11)</td><td>null</td></tr><tr><td>date_name_eu</td><td>char(11)</td><td>null</td></tr><tr><td>day_of_week</td><td>tinyint</td><td>null</td></tr><tr><td>day_name_of_week</td><td>char(10)</td><td>null</td></tr><tr><td>day_of_month</td><td>tinyint</td><td>null</td></tr><tr><td>day_of_year</td><td>int</td><td>null</td></tr><tr><td>weekday_weekend</td><td>char(10)</td><td>null</td></tr><tr><td>week_of_year</td><td>tinyint</td><td>null</td></tr><tr><td>month_name</td><td>char(10)</td><td>null</td></tr><tr><td>month_of_year</td><td>tinyint</td><td>null</td></tr><tr><td>is_last_day_of_month</td><td>char(1)</td><td>null</td></tr><tr><td>calendar_quarter</td><td>tinyint</td><td>null</td></tr><tr><td>calendar_year</td><td>int</td><td>null</td></tr><tr><td>calendar_year_month</td><td>char(10)</td><td>null</td></tr><tr><td>calendar_year_qtr</td><td>char(10)</td><td>null</td></tr><tr><td>fiscal_month_of_year</td><td>tinyint</td><td>null</td></tr><tr><td>fiscal_quarter</td><td>tinyint</td><td>null</td></tr><tr><td>fiscal_year</td><td>int</td><td>null</td></tr><tr><td>fiscal_year_month</td><td>char(10)</td><td>null</td></tr><tr><td>fiscal_year_qtr</td><td>char(10)</td><td>null</td></tr><tr><td></td><td></td><td></td></tr><tr><td># Delta Statistics Columns</td><td></td><td></td></tr><tr><td>Column Names</td><td>month_of_year, weekday_weekend, date_name_us, day_of_week, date_key, calendar_quarter, calendar_year_month, date_name_eu, date_name, fiscal_quarter, is_last_day_of_month, day_of_month, month_name, calendar_year_qtr, fiscal_year_qtr, full_date, fiscal_year_month, fiscal_month_of_year, calendar_year, day_name_of_week, day_of_year, week_of_year, fiscal_year</td><td></td></tr><tr><td>Column Selection Method</td><td>first-32</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>spark_catalog</td><td></td></tr><tr><td>Database</td><td>mystore</td><td></td></tr><tr><td>Table</td><td>dim_date</td><td></td></tr><tr><td>Created Time</td><td>Sun Dec 08 15:12:20 UTC 2024</td><td></td></tr><tr><td>Last Access</td><td>UNKNOWN</td><td></td></tr><tr><td>Created By</td><td>Spark 3.5.2</td><td></td></tr><tr><td>Type</td><td>EXTERNAL</td><td></td></tr><tr><td>Comment</td><td>Date Dimension Table</td><td></td></tr><tr><td>Location</td><td>dbfs:/FileStore/lab_data/mystore/dim_date</td><td></td></tr><tr><td>Provider</td><td>delta</td><td></td></tr><tr><td>Owner</td><td>root</td><td></td></tr><tr><td>Table Properties</td><td>[delta.enableDeletionVectors=true,delta.feature.appendOnly=supported,delta.feature.deletionVectors=supported,delta.feature.invariants=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "date_key",
         "int",
         null
        ],
        [
         "full_date",
         "date",
         null
        ],
        [
         "date_name",
         "char(11)",
         null
        ],
        [
         "date_name_us",
         "char(11)",
         null
        ],
        [
         "date_name_eu",
         "char(11)",
         null
        ],
        [
         "day_of_week",
         "tinyint",
         null
        ],
        [
         "day_name_of_week",
         "char(10)",
         null
        ],
        [
         "day_of_month",
         "tinyint",
         null
        ],
        [
         "day_of_year",
         "int",
         null
        ],
        [
         "weekday_weekend",
         "char(10)",
         null
        ],
        [
         "week_of_year",
         "tinyint",
         null
        ],
        [
         "month_name",
         "char(10)",
         null
        ],
        [
         "month_of_year",
         "tinyint",
         null
        ],
        [
         "is_last_day_of_month",
         "char(1)",
         null
        ],
        [
         "calendar_quarter",
         "tinyint",
         null
        ],
        [
         "calendar_year",
         "int",
         null
        ],
        [
         "calendar_year_month",
         "char(10)",
         null
        ],
        [
         "calendar_year_qtr",
         "char(10)",
         null
        ],
        [
         "fiscal_month_of_year",
         "tinyint",
         null
        ],
        [
         "fiscal_quarter",
         "tinyint",
         null
        ],
        [
         "fiscal_year",
         "int",
         null
        ],
        [
         "fiscal_year_month",
         "char(10)",
         null
        ],
        [
         "fiscal_year_qtr",
         "char(10)",
         null
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Delta Statistics Columns",
         "",
         ""
        ],
        [
         "Column Names",
         "month_of_year, weekday_weekend, date_name_us, day_of_week, date_key, calendar_quarter, calendar_year_month, date_name_eu, date_name, fiscal_quarter, is_last_day_of_month, day_of_month, month_name, calendar_year_qtr, fiscal_year_qtr, full_date, fiscal_year_month, fiscal_month_of_year, calendar_year, day_name_of_week, day_of_year, week_of_year, fiscal_year",
         ""
        ],
        [
         "Column Selection Method",
         "first-32",
         ""
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Detailed Table Information",
         "",
         ""
        ],
        [
         "Catalog",
         "spark_catalog",
         ""
        ],
        [
         "Database",
         "mystore",
         ""
        ],
        [
         "Table",
         "dim_date",
         ""
        ],
        [
         "Created Time",
         "Sun Dec 08 15:12:20 UTC 2024",
         ""
        ],
        [
         "Last Access",
         "UNKNOWN",
         ""
        ],
        [
         "Created By",
         "Spark 3.5.2",
         ""
        ],
        [
         "Type",
         "EXTERNAL",
         ""
        ],
        [
         "Comment",
         "Date Dimension Table",
         ""
        ],
        [
         "Location",
         "dbfs:/FileStore/lab_data/mystore/dim_date",
         ""
        ],
        [
         "Provider",
         "delta",
         ""
        ],
        [
         "Owner",
         "root",
         ""
        ],
        [
         "Table Properties",
         "[delta.enableDeletionVectors=true,delta.feature.appendOnly=supported,delta.feature.deletionVectors=supported,delta.feature.invariants=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 23
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED mystore.dim_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025f36da-9785-4aff-86d6-f5db458e72ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date_key</th><th>full_date</th><th>date_name</th><th>date_name_us</th><th>date_name_eu</th><th>day_of_week</th><th>day_name_of_week</th><th>day_of_month</th><th>day_of_year</th><th>weekday_weekend</th><th>week_of_year</th><th>month_name</th><th>month_of_year</th><th>is_last_day_of_month</th><th>calendar_quarter</th><th>calendar_year</th><th>calendar_year_month</th><th>calendar_year_qtr</th><th>fiscal_month_of_year</th><th>fiscal_quarter</th><th>fiscal_year</th><th>fiscal_year_month</th><th>fiscal_year_qtr</th></tr></thead><tbody><tr><td>20000101</td><td>2000-01-01</td><td>2000/01/01</td><td>01/01/2000</td><td>01/01/2000</td><td>7</td><td>Saturday</td><td>1</td><td>1</td><td>Weekend</td><td>52</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000102</td><td>2000-01-02</td><td>2000/01/02</td><td>01/02/2000</td><td>02/01/2000</td><td>1</td><td>Sunday</td><td>2</td><td>2</td><td>Weekend</td><td>52</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000103</td><td>2000-01-03</td><td>2000/01/03</td><td>01/03/2000</td><td>03/01/2000</td><td>2</td><td>Monday</td><td>3</td><td>3</td><td>Weekday</td><td>1</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000104</td><td>2000-01-04</td><td>2000/01/04</td><td>01/04/2000</td><td>04/01/2000</td><td>3</td><td>Tuesday</td><td>4</td><td>4</td><td>Weekday</td><td>1</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr><tr><td>20000105</td><td>2000-01-05</td><td>2000/01/05</td><td>01/05/2000</td><td>05/01/2000</td><td>4</td><td>Wednesday</td><td>5</td><td>5</td><td>Weekday</td><td>1</td><td>January</td><td>1</td><td>N</td><td>1</td><td>2000</td><td>2000-01</td><td>2000Q1</td><td>7</td><td>3</td><td>2000</td><td>2000-07</td><td>2000Q3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20000101,
         "2000-01-01",
         "2000/01/01",
         "01/01/2000",
         "01/01/2000",
         7,
         "Saturday",
         1,
         1,
         "Weekend",
         52,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000102,
         "2000-01-02",
         "2000/01/02",
         "01/02/2000",
         "02/01/2000",
         1,
         "Sunday",
         2,
         2,
         "Weekend",
         52,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000103,
         "2000-01-03",
         "2000/01/03",
         "01/03/2000",
         "03/01/2000",
         2,
         "Monday",
         3,
         3,
         "Weekday",
         1,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000104,
         "2000-01-04",
         "2000/01/04",
         "01/04/2000",
         "04/01/2000",
         3,
         "Tuesday",
         4,
         4,
         "Weekday",
         1,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ],
        [
         20000105,
         "2000-01-05",
         "2000/01/05",
         "01/05/2000",
         "05/01/2000",
         4,
         "Wednesday",
         5,
         5,
         "Weekday",
         1,
         "January",
         1,
         "N",
         1,
         2000,
         "2000-01",
         "2000Q1",
         7,
         3,
         2000,
         "2000-07",
         "2000Q3"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 25
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "date_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "full_date",
         "type": "\"date\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(11)\",\"signed\":true,\"scale\":0}",
         "name": "date_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(11)\",\"signed\":true,\"scale\":0}",
         "name": "date_name_us",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(11)\",\"signed\":true,\"scale\":0}",
         "name": "date_name_eu",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "day_of_week",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(10)\",\"signed\":true,\"scale\":0}",
         "name": "day_name_of_week",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "day_of_month",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "day_of_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(10)\",\"signed\":true,\"scale\":0}",
         "name": "weekday_weekend",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "week_of_year",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(10)\",\"signed\":true,\"scale\":0}",
         "name": "month_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "month_of_year",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(1)\",\"signed\":true,\"scale\":0}",
         "name": "is_last_day_of_month",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "calendar_quarter",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "calendar_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(10)\",\"signed\":true,\"scale\":0}",
         "name": "calendar_year_month",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(10)\",\"signed\":true,\"scale\":0}",
         "name": "calendar_year_qtr",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "fiscal_month_of_year",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "fiscal_quarter",
         "type": "\"byte\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"signed\":true,\"scale\":0}",
         "name": "fiscal_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(10)\",\"signed\":true,\"scale\":0}",
         "name": "fiscal_year_month",
         "type": "\"string\""
        },
        {
         "metadata": "{\"isTimestampNTZ\":false,\"__CHAR_VARCHAR_TYPE_STRING\":\"char(10)\",\"signed\":true,\"scale\":0}",
         "name": "fiscal_year_qtr",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM mystore.dim_date LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e714247-5e8f-4292-8743-37b4faebc1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1.3. Create a New Table that Sources Product Dimension Data from an Azure MySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838e63d2-0d7e-4850-8ab7-777721398b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE mystore.dim_sales (\n",
    "    row_id INT,\n",
    "    order_id VARCHAR(255),\n",
    "    order_date DATE,\n",
    "    ship_date DATE,\n",
    "    ship_mode VARCHAR(255),\n",
    "    customer_id VARCHAR(255),\n",
    "    customer_name VARCHAR(255),\n",
    "    segment VARCHAR(255),\n",
    "    city VARCHAR(255),\n",
    "    state VARCHAR(255),\n",
    "    country VARCHAR(255),\n",
    "    postal_code VARCHAR(255),\n",
    "    market VARCHAR(255),\n",
    "    region VARCHAR(255),\n",
    "    product_id VARCHAR(255),\n",
    "    category VARCHAR(255),\n",
    "    sub_category VARCHAR(255),\n",
    "    product_name VARCHAR(255),\n",
    "    sales DECIMAL(10, 2),\n",
    "    quantity INT,\n",
    "    discount DECIMAL(10, 2),\n",
    "    profit DECIMAL(10, 2),\n",
    "    shipping_cost DECIMAL(10, 2),\n",
    "    order_priority VARCHAR(255)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c51628-bb99-4846-80dc-b8d85b40eebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in /local_disk0/.ephemeral_nfs/envs/pythonEnv-866f2c56-3264-4234-8df0-dba4ce66fd53/lib/python3.12/site-packages (9.1.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nData successfully loaded to MySQL\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "def upload_json_to_mysql():\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"JSON to MySQL\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Define schema matching your JSON structure\n",
    "    schema = StructType([\n",
    "        StructField(\"Row ID\", StringType(), True),\n",
    "        StructField(\"Order ID\", StringType(), True),\n",
    "        StructField(\"Order Date\", StringType(), True),\n",
    "        StructField(\"Ship Date\", StringType(), True),\n",
    "        StructField(\"Ship Mode\", StringType(), True),\n",
    "        StructField(\"Customer ID\", StringType(), True),\n",
    "        StructField(\"Customer Name\", StringType(), True),\n",
    "        StructField(\"Segment\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True),\n",
    "        StructField(\"Country\", StringType(), True),\n",
    "        StructField(\"Postal Code\", StringType(), True),\n",
    "        StructField(\"Market\", StringType(), True),\n",
    "        StructField(\"Region\", StringType(), True),\n",
    "        StructField(\"Product ID\", StringType(), True),\n",
    "        StructField(\"Category\", StringType(), True),\n",
    "        StructField(\"Sub-Category\", StringType(), True),\n",
    "        StructField(\"Product Name\", StringType(), True),\n",
    "        StructField(\"Sales\", DecimalType(10,2), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"Discount\", DecimalType(4,2), True),\n",
    "        StructField(\"Profit\", DecimalType(10,2), True),\n",
    "        StructField(\"Shipping Cost\", DecimalType(10,2), True),\n",
    "        StructField(\"Order Priority\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read JSON file\n",
    "    df = spark.read \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .json(\"dbfs:/FileStore/shared_uploads/upn2vz@virginia.edu/StoreSales.json send it to me by 10 that would be perfect 10 or\")\n",
    "    \n",
    "    # MySQL connection properties\n",
    "    mysql_properties = {\n",
    "        \"user\": \"upn2vzatvirginia\",\n",
    "        \"password\": \"Akinolami6650!\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "\n",
    "    # MySQL URL\n",
    "    mysql_url = \"jdbc:mysql://upn2vz-mysql.mysql.database.azure.com:3306/mystore?useSSL=true\"\n",
    "    \n",
    "    # Write to MySQL\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .jdbc(url=mysql_url,\n",
    "              table=\"source_sales\",\n",
    "              properties=mysql_properties)\n",
    "    \n",
    "    print(\"Data successfully loaded to MySQL\")\n",
    "\n",
    "upload_json_to_mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0013f0a6-aa02-4a88-b6db-08af0659ea8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: java.sql.SQLSyntaxErrorException: (conn=59) Table 'mystore.dim_sales' doesn't exist\n",
       "\tat org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:62)\n",
       "\tat org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:158)\n",
       "\tat org.mariadb.jdbc.MariaDbStatement.executeExceptionEpilogue(MariaDbStatement.java:262)\n",
       "\tat org.mariadb.jdbc.ClientSidePreparedStatement.executeInternal(ClientSidePreparedStatement.java:229)\n",
       "\tat org.mariadb.jdbc.ClientSidePreparedStatement.execute(ClientSidePreparedStatement.java:149)\n",
       "\tat org.mariadb.jdbc.ClientSidePreparedStatement.executeQuery(ClientSidePreparedStatement.java:163)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:76)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$1(JDBCRDD.scala:74)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:68)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:253)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:44)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:401)\n",
       "\tat org.apache.spark.sql.execution.datasources.CreateTempViewUsing.$anonfun$run$2(ddl.scala:135)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.datasources.CreateTempViewUsing.run(ddl.scala:127)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandWithAetherOff(SparkPlan.scala:181)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:192)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:387)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:387)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:193)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:387)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:453)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:738)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:334)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:205)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:675)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:383)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1125)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:379)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:329)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:377)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:431)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:426)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:426)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:426)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:288)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:285)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:383)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:132)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1280)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1280)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1051)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1015)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1075)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:311)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:411)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:434)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:429)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1146)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1194)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1185)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:103)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:103)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1120)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1559)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:787)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:1033)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:1022)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:1068)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:90)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:606)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:516)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:90)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:1068)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:756)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:863)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:628)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:618)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:325)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Table 'mystore.dim_sales' doesn't exist\n",
       "\tat org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)\n",
       "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:195)\n",
       "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:178)\n",
       "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:322)\n",
       "\tat org.mariadb.jdbc.ClientSidePreparedStatement.executeInternal(ClientSidePreparedStatement.java:220)\n",
       "\t... 122 more\n",
       "Caused by: java.sql.SQLException: Table 'mystore.dim_sales' doesn't exist\n",
       "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1693)\n",
       "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1555)\n",
       "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1518)\n",
       "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:319)\n",
       "\t... 123 more\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:486)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1146)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1194)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1185)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:103)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:103)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1120)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1559)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:787)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:1033)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:1022)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:1068)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:90)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:606)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:516)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:90)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:1068)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:756)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:863)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:628)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:618)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:325)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "SQLSyntaxErrorException: (conn=59) Table 'mystore.dim_sales' doesn't exist"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: java.sql.SQLSyntaxErrorException: (conn=59) Table 'mystore.dim_sales' doesn't exist",
        "\tat org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:62)",
        "\tat org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:158)",
        "\tat org.mariadb.jdbc.MariaDbStatement.executeExceptionEpilogue(MariaDbStatement.java:262)",
        "\tat org.mariadb.jdbc.ClientSidePreparedStatement.executeInternal(ClientSidePreparedStatement.java:229)",
        "\tat org.mariadb.jdbc.ClientSidePreparedStatement.execute(ClientSidePreparedStatement.java:149)",
        "\tat org.mariadb.jdbc.ClientSidePreparedStatement.executeQuery(ClientSidePreparedStatement.java:163)",
        "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:76)",
        "\tat scala.util.Using$.resource(Using.scala:269)",
        "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$1(JDBCRDD.scala:74)",
        "\tat scala.util.Using$.resource(Using.scala:269)",
        "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:73)",
        "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:68)",
        "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:253)",
        "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:44)",
        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:401)",
        "\tat org.apache.spark.sql.execution.datasources.CreateTempViewUsing.$anonfun$run$2(ddl.scala:135)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat org.apache.spark.sql.execution.datasources.CreateTempViewUsing.run(ddl.scala:127)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)",
        "\tat org.apache.spark.sql.execution.SparkPlan.runCommandWithAetherOff(SparkPlan.scala:181)",
        "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:192)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)",
        "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)",
        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:387)",
        "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)",
        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:387)",
        "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:193)",
        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:387)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:453)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:738)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:334)",
        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:205)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:675)",
        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:383)",
        "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1125)",
        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:379)",
        "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:329)",
        "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:377)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:431)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:426)",
        "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505)",
        "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)",
        "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:40)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)",
        "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481)",
        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:426)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)",
        "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:426)",
        "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:288)",
        "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:285)",
        "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:383)",
        "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:132)",
        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)",
        "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1280)",
        "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)",
        "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1280)",
        "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)",
        "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1051)",
        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)",
        "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1015)",
        "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1075)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:311)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:411)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:434)",
        "\tat scala.collection.immutable.List.map(List.scala:293)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:429)",
        "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1146)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1194)",
        "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1185)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:103)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:103)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1120)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1559)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:787)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:1033)",
        "\tat scala.util.Try$.apply(Try.scala:213)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:1022)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:1068)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:631)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:90)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:606)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:516)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:90)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:1068)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:756)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:863)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:628)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:618)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:325)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)",
        "Caused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Table 'mystore.dim_sales' doesn't exist",
        "\tat org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)",
        "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:195)",
        "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:178)",
        "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:322)",
        "\tat org.mariadb.jdbc.ClientSidePreparedStatement.executeInternal(ClientSidePreparedStatement.java:220)",
        "\t... 122 more",
        "Caused by: java.sql.SQLException: Table 'mystore.dim_sales' doesn't exist",
        "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1693)",
        "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1555)",
        "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1518)",
        "\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.executeQuery(AbstractQueryProtocol.java:319)",
        "\t... 123 more",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:486)",
        "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1146)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$35(DriverLocal.scala:1194)",
        "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$29(DriverLocal.scala:1185)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:103)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:103)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1120)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1559)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:787)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:1033)",
        "\tat scala.util.Try$.apply(Try.scala:213)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:1022)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:1068)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:631)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:90)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:606)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:516)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:90)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:1068)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:756)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:863)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:628)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:271)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:267)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:90)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:618)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:325)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_sales\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"jdbc:mysql://upn2vz-mysql.mysql.database.azure.com:3306/mystore?useSSL=true&requireSSL=true\",\n",
    "  dbtable \"dim_sales\",\n",
    "  user \"upn2vzatvirginia\",\n",
    "  password \"Akinolami6650!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc5da2c-3ce8-4051-9c7b-a517ef493268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS sales_dw\n",
    "COMMENT \"Sales Data Warehouse\"\n",
    "LOCATION \"dbfs:/FileStore/sales_dw\"\n",
    "WITH DBPROPERTIES ('contains_pii' = 'true', 'purpose' = 'Sales Analysis')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f6f553-8008-47e1-883e-9221e3d704fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def setup_sales_stream():\n",
    "    return (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{output_bronze}/sales_schema\")\n",
    "        .load(sales_stream_dir)\n",
    "        .createOrReplaceTempView(\"sales_bronze_view\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16981cb9-2c51-4e40-9a9c-48780c63950c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def setup_product_stream():\n",
    "    return (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{output_bronze}/products_schema\")\n",
    "        .load(api_stream_dir)\n",
    "        .createOrReplaceTempView(\"products_bronze_view\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29967a07-3b84-4670-a7ef-11b2133d5dda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2024-12-08 14:56:49,333\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_bronze_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\", \"context\": {\"error_class\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o383.sql.\\n: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_bronze_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5;\\n'CreateViewCommand `sales_silver_view`, SELECT \\n    s.*,\\n    d.year,\\n    d.quarter,\\n    d.month,\\n    c.customer_name,\\n    c.segment,\\n    p.product_name,\\n    p.category\\nFROM sales_bronze_view s\\nJOIN dim_date d ON s.order_date_id = d.date_id\\nJOIN dim_customer c ON s.customer_id = c.customer_id\\nJOIN dim_product p ON s.product_id = p.product_id, false, true, LocalTempView, UNSUPPORTED, false\\n+- 'Project [s.*, 'd.year, 'd.quarter, 'd.month, 'c.customer_name, 'c.segment, 'p.product_name, 'p.category]\\n   +- 'Join Inner, ('s.product_id = 'p.product_id)\\n      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\\n      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\\n      :  :  :- 'SubqueryAlias s\\n      :  :  :  +- 'UnresolvedRelation [sales_bronze_view], [], false\\n      :  :  +- 'SubqueryAlias d\\n      :  :     +- 'UnresolvedRelation [dim_date], [], false\\n      :  +- 'SubqueryAlias c\\n      :     +- 'UnresolvedRelation [dim_customer], [], false\\n      +- 'SubqueryAlias p\\n         +- 'UnresolvedRelation [dim_product], [], false\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:258)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.immutable.List.foreach(List.scala:431)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:213)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:388)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:198)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:185)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:185)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:388)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:443)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:193)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:443)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:440)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:264)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:472)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:562)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:562)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1125)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:561)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:557)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:557)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:258)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:257)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:239)\\n\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1280)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1280)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:969)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:933)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:992)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\\n\\tat py4j.Gateway.invoke(Gateway.java:306)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/databricks/spark/python/pyspark/errors/exceptions/captured.py\\\", line 263, in deco\", \"    return f(*a, **kw)\", \"           ^^^^^^^^^^^\", \"  File \\\"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\\\", line 326, in get_return_value\", \"    raise Py4JJavaError(\", \"py4j.protocol.Py4JJavaError: An error occurred while calling o383.sql.\", \": org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_bronze_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\", \"If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\", \"To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5;\", \"'CreateViewCommand `sales_silver_view`, SELECT \", \"    s.*,\", \"    d.year,\", \"    d.quarter,\", \"    d.month,\", \"    c.customer_name,\", \"    c.segment,\", \"    p.product_name,\", \"    p.category\", \"FROM sales_bronze_view s\", \"JOIN dim_date d ON s.order_date_id = d.date_id\", \"JOIN dim_customer c ON s.customer_id = c.customer_id\", \"JOIN dim_product p ON s.product_id = p.product_id, false, true, LocalTempView, UNSUPPORTED, false\", \"+- 'Project [s.*, 'd.year, 'd.quarter, 'd.month, 'c.customer_name, 'c.segment, 'p.product_name, 'p.category]\", \"   +- 'Join Inner, ('s.product_id = 'p.product_id)\", \"      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\", \"      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\", \"      :  :  :- 'SubqueryAlias s\", \"      :  :  :  +- 'UnresolvedRelation [sales_bronze_view], [], false\", \"      :  :  +- 'SubqueryAlias d\", \"      :  :     +- 'UnresolvedRelation [dim_date], [], false\", \"      :  +- 'SubqueryAlias c\", \"      :     +- 'UnresolvedRelation [dim_customer], [], false\", \"      +- 'SubqueryAlias p\", \"         +- 'UnresolvedRelation [dim_product], [], false\", \"\", \"\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:258)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:231)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.immutable.List.foreach(List.scala:431)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:231)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:213)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:388)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:198)\", \"\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:185)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:185)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:388)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:443)\", \"\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:193)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:443)\", \"\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:440)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:264)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:472)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:562)\", \"\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:562)\", \"\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1125)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:561)\", \"\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:557)\", \"\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:557)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:258)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:257)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:239)\", \"\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)\", \"\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\", \"\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1280)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1280)\", \"\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\", \"\\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:969)\", \"\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\", \"\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:933)\", \"\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:992)\", \"\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\", \"\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\", \"\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\", \"\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\", \"\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\", \"\\tat py4j.Gateway.invoke(Gateway.java:306)\", \"\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\", \"\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\", \"\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\", \"\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\", \"\\tat java.base/java.lang.Thread.run(Thread.java:840)\"]}}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1915622986156474>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 5. Create Silver Layer with Dimension Joins\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124mCREATE OR REPLACE TEMPORARY VIEW sales_silver_view AS\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124mSELECT \u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124m    s.*,\u001B[39m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124m    d.year,\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124m    d.quarter,\u001B[39m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124m    d.month,\u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124m    c.customer_name,\u001B[39m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124m    c.segment,\u001B[39m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124m    p.product_name,\u001B[39m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124m    p.category\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;124mFROM sales_bronze_view s\u001B[39m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;124mJOIN dim_date d ON s.order_date_id = d.date_id\u001B[39m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;124mJOIN dim_customer c ON s.customer_id = c.customer_id\u001B[39m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124mJOIN dim_product p ON s.product_id = p.product_id\u001B[39m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1870\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1866\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   1867\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINVALID_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1868\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(args)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   1869\u001B[0m         )\n",
       "\u001B[0;32m-> 1870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39msql(sqlQuery, litArgs), \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1871\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1872\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_bronze_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5;\n",
       "'CreateViewCommand `sales_silver_view`, SELECT \n",
       "    s.*,\n",
       "    d.year,\n",
       "    d.quarter,\n",
       "    d.month,\n",
       "    c.customer_name,\n",
       "    c.segment,\n",
       "    p.product_name,\n",
       "    p.category\n",
       "FROM sales_bronze_view s\n",
       "JOIN dim_date d ON s.order_date_id = d.date_id\n",
       "JOIN dim_customer c ON s.customer_id = c.customer_id\n",
       "JOIN dim_product p ON s.product_id = p.product_id, false, true, LocalTempView, UNSUPPORTED, false\n",
       "+- 'Project [s.*, 'd.year, 'd.quarter, 'd.month, 'c.customer_name, 'c.segment, 'p.product_name, 'p.category]\n",
       "   +- 'Join Inner, ('s.product_id = 'p.product_id)\n",
       "      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\n",
       "      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\n",
       "      :  :  :- 'SubqueryAlias s\n",
       "      :  :  :  +- 'UnresolvedRelation [sales_bronze_view], [], false\n",
       "      :  :  +- 'SubqueryAlias d\n",
       "      :  :     +- 'UnresolvedRelation [dim_date], [], false\n",
       "      :  +- 'SubqueryAlias c\n",
       "      :     +- 'UnresolvedRelation [dim_customer], [], false\n",
       "      +- 'SubqueryAlias p\n",
       "         +- 'UnresolvedRelation [dim_product], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_bronze_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5;\n'CreateViewCommand `sales_silver_view`, SELECT \n    s.*,\n    d.year,\n    d.quarter,\n    d.month,\n    c.customer_name,\n    c.segment,\n    p.product_name,\n    p.category\nFROM sales_bronze_view s\nJOIN dim_date d ON s.order_date_id = d.date_id\nJOIN dim_customer c ON s.customer_id = c.customer_id\nJOIN dim_product p ON s.product_id = p.product_id, false, true, LocalTempView, UNSUPPORTED, false\n+- 'Project [s.*, 'd.year, 'd.quarter, 'd.month, 'c.customer_name, 'c.segment, 'p.product_name, 'p.category]\n   +- 'Join Inner, ('s.product_id = 'p.product_id)\n      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\n      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\n      :  :  :- 'SubqueryAlias s\n      :  :  :  +- 'UnresolvedRelation [sales_bronze_view], [], false\n      :  :  +- 'SubqueryAlias d\n      :  :     +- 'UnresolvedRelation [dim_date], [], false\n      :  +- 'SubqueryAlias c\n      :     +- 'UnresolvedRelation [dim_customer], [], false\n      +- 'SubqueryAlias p\n         +- 'UnresolvedRelation [dim_product], [], false\n"
       },
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_bronze_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "42P01",
        "stackTrace": null,
        "startIndex": 188,
        "stopIndex": 204
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-1915622986156474>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 5. Create Silver Layer with Dimension Joins\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124mCREATE OR REPLACE TEMPORARY VIEW sales_silver_view AS\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124mSELECT \u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m    s.*,\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m    d.year,\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m    d.quarter,\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m    d.month,\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m    c.customer_name,\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m    c.segment,\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m    p.product_name,\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m    p.category\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124mFROM sales_bronze_view s\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124mJOIN dim_date d ON s.order_date_id = d.date_id\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124mJOIN dim_customer c ON s.customer_id = c.customer_id\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124mJOIN dim_product p ON s.product_id = p.product_id\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1870\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1866\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   1867\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINVALID_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1868\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(args)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   1869\u001B[0m         )\n\u001B[0;32m-> 1870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39msql(sqlQuery, litArgs), \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1871\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1872\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_bronze_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5;\n'CreateViewCommand `sales_silver_view`, SELECT \n    s.*,\n    d.year,\n    d.quarter,\n    d.month,\n    c.customer_name,\n    c.segment,\n    p.product_name,\n    p.category\nFROM sales_bronze_view s\nJOIN dim_date d ON s.order_date_id = d.date_id\nJOIN dim_customer c ON s.customer_id = c.customer_id\nJOIN dim_product p ON s.product_id = p.product_id, false, true, LocalTempView, UNSUPPORTED, false\n+- 'Project [s.*, 'd.year, 'd.quarter, 'd.month, 'c.customer_name, 'c.segment, 'p.product_name, 'p.category]\n   +- 'Join Inner, ('s.product_id = 'p.product_id)\n      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\n      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\n      :  :  :- 'SubqueryAlias s\n      :  :  :  +- 'UnresolvedRelation [sales_bronze_view], [], false\n      :  :  +- 'SubqueryAlias d\n      :  :     +- 'UnresolvedRelation [dim_date], [], false\n      :  +- 'SubqueryAlias c\n      :     +- 'UnresolvedRelation [dim_customer], [], false\n      +- 'SubqueryAlias p\n         +- 'UnresolvedRelation [dim_product], [], false\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Create Silver Layer with Dimension Joins\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_silver_view AS\n",
    "SELECT \n",
    "    s.*,\n",
    "    d.year,\n",
    "    d.quarter,\n",
    "    d.month,\n",
    "    c.customer_name,\n",
    "    c.segment,\n",
    "    p.product_name,\n",
    "    p.category\n",
    "FROM sales_bronze_view s\n",
    "JOIN dim_date d ON s.order_date_id = d.date_id\n",
    "JOIN dim_customer c ON s.customer_id = c.customer_id\n",
    "JOIN dim_product p ON s.product_id = p.product_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4615f2-0c09-4992-bb13-98268316087f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2024-12-08 14:56:58,498\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_silver_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\", \"context\": {\"error_class\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o383.sql.\\n: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_silver_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5;\\n'ReplaceTableAsSelect TableSpec(Map(),None,Map(),None,None,None,false,Set(),None,None,None), true, false\\n:- ResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@36c67795, sales_dw.fact_sales_metrics\\n+- 'Aggregate ['d.year, 'd.quarter, 'c.segment, 'p.category], ['d.year, 'd.quarter, 'c.segment, 'p.category, 'COUNT(distinct 's.order_id) AS total_orders#3713, 'SUM('s.quantity) AS total_units#3714, 'SUM('s.sales_amount) AS total_sales#3715, 'AVG('s.profit) AS avg_profit#3716, 'STDDEV('s.sales_amount) AS sales_std_dev#3717]\\n   +- 'Join Inner, ('s.product_id = 'p.product_id)\\n      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\\n      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\\n      :  :  :- 'SubqueryAlias s\\n      :  :  :  +- 'UnresolvedRelation [sales_silver_view], [], false\\n      :  :  +- 'SubqueryAlias d\\n      :  :     +- 'UnresolvedRelation [dim_date], [], false\\n      :  +- 'SubqueryAlias c\\n      :     +- 'UnresolvedRelation [dim_customer], [], false\\n      +- 'SubqueryAlias p\\n         +- 'UnresolvedRelation [dim_product], [], false\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:258)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\\n\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\\n\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\\n\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\\n\\tat scala.collection.immutable.List.foreach(List.scala:431)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:213)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:388)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:198)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:185)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:185)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:388)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:443)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:193)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:443)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:440)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:264)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:472)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:562)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:562)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1125)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:561)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:557)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:557)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:258)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:257)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:239)\\n\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1280)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1280)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:969)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:933)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:992)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\\n\\tat py4j.Gateway.invoke(Gateway.java:306)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/databricks/spark/python/pyspark/errors/exceptions/captured.py\\\", line 263, in deco\", \"    return f(*a, **kw)\", \"           ^^^^^^^^^^^\", \"  File \\\"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\\\", line 326, in get_return_value\", \"    raise Py4JJavaError(\", \"py4j.protocol.Py4JJavaError: An error occurred while calling o383.sql.\", \": org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_silver_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\", \"If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\", \"To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5;\", \"'ReplaceTableAsSelect TableSpec(Map(),None,Map(),None,None,None,false,Set(),None,None,None), true, false\", \":- ResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@36c67795, sales_dw.fact_sales_metrics\", \"+- 'Aggregate ['d.year, 'd.quarter, 'c.segment, 'p.category], ['d.year, 'd.quarter, 'c.segment, 'p.category, 'COUNT(distinct 's.order_id) AS total_orders#3713, 'SUM('s.quantity) AS total_units#3714, 'SUM('s.sales_amount) AS total_sales#3715, 'AVG('s.profit) AS avg_profit#3716, 'STDDEV('s.sales_amount) AS sales_std_dev#3717]\", \"   +- 'Join Inner, ('s.product_id = 'p.product_id)\", \"      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\", \"      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\", \"      :  :  :- 'SubqueryAlias s\", \"      :  :  :  +- 'UnresolvedRelation [sales_silver_view], [], false\", \"      :  :  +- 'SubqueryAlias d\", \"      :  :     +- 'UnresolvedRelation [dim_date], [], false\", \"      :  +- 'SubqueryAlias c\", \"      :     +- 'UnresolvedRelation [dim_customer], [], false\", \"      +- 'SubqueryAlias p\", \"         +- 'UnresolvedRelation [dim_product], [], false\", \"\", \"\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:258)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:231)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:287)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.Iterator.foreach(Iterator.scala:943)\", \"\\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\", \"\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\", \"\\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\", \"\\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\", \"\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:286)\", \"\\tat scala.collection.immutable.List.foreach(List.scala:431)\", \"\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:286)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:231)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:213)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:388)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:198)\", \"\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:185)\", \"\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:185)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:388)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:443)\", \"\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\", \"\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:193)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:443)\", \"\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\", \"\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:440)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:264)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:472)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:562)\", \"\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:562)\", \"\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1125)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:561)\", \"\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:557)\", \"\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:557)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:258)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:257)\", \"\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:239)\", \"\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)\", \"\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\", \"\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1280)\", \"\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\", \"\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1280)\", \"\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:123)\", \"\\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:969)\", \"\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1273)\", \"\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:933)\", \"\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:992)\", \"\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\", \"\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\", \"\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\", \"\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\", \"\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\", \"\\tat py4j.Gateway.invoke(Gateway.java:306)\", \"\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\", \"\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\", \"\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\", \"\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\", \"\\tat java.base/java.lang.Thread.run(Thread.java:840)\"]}}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1915622986156475>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 6. Create Gold Layer Aggregations\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124mCREATE OR REPLACE TABLE sales_dw.fact_sales_metrics AS\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124mSELECT \u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124m    d.year,\u001B[39m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124m    d.quarter,\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124m    c.segment,\u001B[39m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124m    p.category,\u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124m    COUNT(DISTINCT s.order_id) as total_orders,\u001B[39m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124m    SUM(s.quantity) as total_units,\u001B[39m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124m    SUM(s.sales_amount) as total_sales,\u001B[39m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124m    AVG(s.profit) as avg_profit,\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;124m    STDDEV(s.sales_amount) as sales_std_dev\u001B[39m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;124mFROM sales_silver_view s\u001B[39m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;124mJOIN dim_date d ON s.order_date_id = d.date_id\u001B[39m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124mJOIN dim_customer c ON s.customer_id = c.customer_id\u001B[39m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124mJOIN dim_product p ON s.product_id = p.product_id\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;124mGROUP BY d.year, d.quarter, c.segment, p.category\u001B[39m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1870\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1866\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   1867\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINVALID_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1868\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(args)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   1869\u001B[0m         )\n",
       "\u001B[0;32m-> 1870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39msql(sqlQuery, litArgs), \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1871\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1872\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_silver_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5;\n",
       "'ReplaceTableAsSelect TableSpec(Map(),None,Map(),None,None,None,false,Set(),None,None,None), true, false\n",
       ":- ResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@36c67795, sales_dw.fact_sales_metrics\n",
       "+- 'Aggregate ['d.year, 'd.quarter, 'c.segment, 'p.category], ['d.year, 'd.quarter, 'c.segment, 'p.category, 'COUNT(distinct 's.order_id) AS total_orders#3713, 'SUM('s.quantity) AS total_units#3714, 'SUM('s.sales_amount) AS total_sales#3715, 'AVG('s.profit) AS avg_profit#3716, 'STDDEV('s.sales_amount) AS sales_std_dev#3717]\n",
       "   +- 'Join Inner, ('s.product_id = 'p.product_id)\n",
       "      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\n",
       "      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\n",
       "      :  :  :- 'SubqueryAlias s\n",
       "      :  :  :  +- 'UnresolvedRelation [sales_silver_view], [], false\n",
       "      :  :  +- 'SubqueryAlias d\n",
       "      :  :     +- 'UnresolvedRelation [dim_date], [], false\n",
       "      :  +- 'SubqueryAlias c\n",
       "      :     +- 'UnresolvedRelation [dim_customer], [], false\n",
       "      +- 'SubqueryAlias p\n",
       "         +- 'UnresolvedRelation [dim_product], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_silver_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5;\n'ReplaceTableAsSelect TableSpec(Map(),None,Map(),None,None,None,false,Set(),None,None,None), true, false\n:- ResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@36c67795, sales_dw.fact_sales_metrics\n+- 'Aggregate ['d.year, 'd.quarter, 'c.segment, 'p.category], ['d.year, 'd.quarter, 'c.segment, 'p.category, 'COUNT(distinct 's.order_id) AS total_orders#3713, 'SUM('s.quantity) AS total_units#3714, 'SUM('s.sales_amount) AS total_sales#3715, 'AVG('s.profit) AS avg_profit#3716, 'STDDEV('s.sales_amount) AS sales_std_dev#3717]\n   +- 'Join Inner, ('s.product_id = 'p.product_id)\n      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\n      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\n      :  :  :- 'SubqueryAlias s\n      :  :  :  +- 'UnresolvedRelation [sales_silver_view], [], false\n      :  :  +- 'SubqueryAlias d\n      :  :     +- 'UnresolvedRelation [dim_date], [], false\n      :  +- 'SubqueryAlias c\n      :     +- 'UnresolvedRelation [dim_customer], [], false\n      +- 'SubqueryAlias p\n         +- 'UnresolvedRelation [dim_product], [], false\n"
       },
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_silver_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "42P01",
        "stackTrace": null,
        "startIndex": 328,
        "stopIndex": 344
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-1915622986156475>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 6. Create Gold Layer Aggregations\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124mCREATE OR REPLACE TABLE sales_dw.fact_sales_metrics AS\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124mSELECT \u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m    d.year,\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m    d.quarter,\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m    c.segment,\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m    p.category,\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m    COUNT(DISTINCT s.order_id) as total_orders,\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m    SUM(s.quantity) as total_units,\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m    SUM(s.sales_amount) as total_sales,\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m    AVG(s.profit) as avg_profit,\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124m    STDDEV(s.sales_amount) as sales_std_dev\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124mFROM sales_silver_view s\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124mJOIN dim_date d ON s.order_date_id = d.date_id\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124mJOIN dim_customer c ON s.customer_id = c.customer_id\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124mJOIN dim_product p ON s.product_id = p.product_id\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124mGROUP BY d.year, d.quarter, c.segment, p.category\u001B[39m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1870\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1865\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1866\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   1867\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINVALID_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1868\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(args)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   1869\u001B[0m         )\n\u001B[0;32m-> 1870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39msql(sqlQuery, litArgs), \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1871\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1872\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:269\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    265\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sales_silver_view` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5;\n'ReplaceTableAsSelect TableSpec(Map(),None,Map(),None,None,None,false,Set(),None,None,None), true, false\n:- ResolvedIdentifier com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@36c67795, sales_dw.fact_sales_metrics\n+- 'Aggregate ['d.year, 'd.quarter, 'c.segment, 'p.category], ['d.year, 'd.quarter, 'c.segment, 'p.category, 'COUNT(distinct 's.order_id) AS total_orders#3713, 'SUM('s.quantity) AS total_units#3714, 'SUM('s.sales_amount) AS total_sales#3715, 'AVG('s.profit) AS avg_profit#3716, 'STDDEV('s.sales_amount) AS sales_std_dev#3717]\n   +- 'Join Inner, ('s.product_id = 'p.product_id)\n      :- 'Join Inner, ('s.customer_id = 'c.customer_id)\n      :  :- 'Join Inner, ('s.order_date_id = 'd.date_id)\n      :  :  :- 'SubqueryAlias s\n      :  :  :  +- 'UnresolvedRelation [sales_silver_view], [], false\n      :  :  +- 'SubqueryAlias d\n      :  :     +- 'UnresolvedRelation [dim_date], [], false\n      :  +- 'SubqueryAlias c\n      :     +- 'UnresolvedRelation [dim_customer], [], false\n      +- 'SubqueryAlias p\n         +- 'UnresolvedRelation [dim_product], [], false\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Create Gold Layer Aggregations\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE sales_dw.fact_sales_metrics AS\n",
    "SELECT \n",
    "    d.year,\n",
    "    d.quarter,\n",
    "    c.segment,\n",
    "    p.category,\n",
    "    COUNT(DISTINCT s.order_id) as total_orders,\n",
    "    SUM(s.quantity) as total_units,\n",
    "    SUM(s.sales_amount) as total_sales,\n",
    "    AVG(s.profit) as avg_profit,\n",
    "    STDDEV(s.sales_amount) as sales_std_dev\n",
    "FROM sales_silver_view s\n",
    "JOIN dim_date d ON s.order_date_id = d.date_id\n",
    "JOIN dim_customer c ON s.customer_id = c.customer_id\n",
    "JOIN dim_product p ON s.product_id = p.product_id\n",
    "GROUP BY d.year, d.quarter, c.segment, p.category\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74b7fdce-ad94-4529-8e34-3a6a73ea4ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Start Streaming Query\n",
    "def start_streaming():\n",
    "    return (spark.table(\"sales_silver_view\")\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{output_silver}/checkpoint\")\n",
    "        .table(\"sales_dw.fact_sales\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4c1366-a886-43bd-bc1e-4b48b4fe0f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Analysis Views\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW sales_dw.vw_customer_analysis AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    segment,\n",
    "    SUM(sales_amount) as total_sales,\n",
    "    COUNT(DISTINCT order_id) as order_count,\n",
    "    AVG(sales_amount) as avg_order_value,\n",
    "    STDDEV(sales_amount) as order_value_std_dev\n",
    "FROM sales_dw.fact_sales\n",
    "GROUP BY customer_id, customer_name, segment\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1915622986156496,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone Project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
